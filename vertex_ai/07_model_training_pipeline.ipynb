{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eebfad-85ab-4c8e-a04d-7a05d9b59af7",
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277225ec-bfdd-448f-8d70-9714504f6f6b",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Fraudfinder - ML Pipeline\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/fraudfinder/raw/main/06_model_training_pipeline.ipynb\">\n",
    "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Google Cloud Notebooks\">Open in Cloud Notebook\n",
    "    </a>\n",
    "  </td> \n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/fraudfinder/blob/main/06_model_training_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/fraudfinder/blob/main/06_model_training_pipeline.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e9bea-5791-42cd-a966-4cb06d0026dc",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Fraudfinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the Fraudfinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7bc82-36ae-4673-8ee2-3403a9d9806f",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This notebook shows how to use Feature Store, Pipelines and Model Monitoring for building an end-to-end demo using both components defined in `google_cloud_pipeline_components` and custom components. \n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "- [BigQuery](https://cloud.google.com/bigquery/)\n",
    "\n",
    "Steps performed in this notebook:\n",
    "\n",
    "    * Create a Feature Store for store and sharing features\n",
    "    * Create a Pipeline to deploy the model\n",
    "    * Create a Model Monitoring Job to check the status of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12dbb1-b84e-4e22-b8c5-badfac653a4d",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4599a9-9167-422a-ab25-0af1ffadda84",
   "metadata": {},
   "source": [
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d7ba4-454c-4ff8-a977-3078f0ee3e24",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb9ddec-51fd-44f8-b200-4d17a08800ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dfc18f-14cc-406a-b352-fb8cfdbfd387",
   "metadata": {
    "id": "XoEqT2Y4DJmf",
    "tags": []
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e0786-4cf7-4967-a88f-55ab302e3660",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ac8d3f-5ea7-45e2-add9-fc63e7b03d9c",
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddddc45b-3d7e-427c-b2e7-65a3b07c1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DISPLAY_NAME = f'xgboost-fruad-finder'\n",
    "PIPELINE_NAME = f'fraud-finder-xgb-pipeline-{ID}'\n",
    "PIPELINE_STORE_URI = f\"{BUCKET_NAME}/pipeline-store/\"\n",
    "IMAGE_REPOSITORY = f'fraudfinder-{ID}'\n",
    "IMAGE_NAME='dask-xgb-classificator'\n",
    "IMAGE_TAG='v1'\n",
    "IMAGE_URI=f\"us-central1-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f697e-1fe8-4a48-ae1f-56c82e4a91d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./pipelines/components/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5541c483-4592-4f14-aade-c169a7e3325a",
   "metadata": {},
   "source": [
    "### Define Custom Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aac2ec-2e40-4d75-aa56-6bbfb4cb3291",
   "metadata": {},
   "source": [
    "#### Define feature store component\n",
    "\n",
    "Notice that the component assumes that containes the entities-timestamps \"query\" is already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06ea2b-3e59-4715-8fe7-69f97c2916cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile deploy_kfp_pipeline/pipeline/components/batch_serve.py\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Component for interacting with the feature store\n",
    "\"\"\"\n",
    "\n",
    "from kfp.v2.dsl import Dataset, Input, Output, Artifact, component\n",
    "from typing import NamedTuple\n",
    "\n",
    "# COMPONENTS_DIR=os.path.join(os.curdir, 'pipelines', 'components')\n",
    "# COMPONENT_URI=f\"{COMPONENTS_DIR}/features_to_gcs.yaml\"\n",
    "\n",
    "@component(output_component_file='./pipelines/components/features_to_gcs.yaml', \n",
    "       base_image='python:3.7', \n",
    "       packages_to_install=[\"git+https://github.com/googleapis/python-aiplatform.git@main\"])\n",
    "\n",
    "def features_to_gcs(project_id:str, region:str, bucket_name:str, feature_store_id: str, read_instances_uri:str) -> NamedTuple(\"Outputs\", [(\"snapshot_uri_paths\", str),],):\n",
    "\n",
    "    # Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "    from datetime import datetime\n",
    "    import glob\n",
    "    import urllib\n",
    "    import json\n",
    "\n",
    "    #Feature Store\n",
    "    from google.cloud.aiplatform import Featurestore, EntityType, Feature\n",
    "\n",
    "    # Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    api_endpoint = region + \"-aiplatform.googleapis.com\"\n",
    "    bucket = urllib.parse.urlsplit(bucket_name).netloc\n",
    "    export_uri = f'{bucket_name}/data/snapshots/{timestamp}' #format as new gsfuse requires\n",
    "    export_uri_path = f'/gcs/{bucket}/data/snapshots/{timestamp}' \n",
    "    event_entity = 'event'\n",
    "    customer_entity = 'customer'\n",
    "    terminal_entity = 'terminal'\n",
    "    serving_feature_ids = {customer_entity: ['*'], terminal_entity: ['*']}\n",
    "\n",
    "    # Main -------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    ## Set a client for Feature store managment\n",
    "\n",
    "    ### Create admin_client for create, read, update and delete (CRUD)\n",
    "    feature_store_resource_path = f\"projects/{project_id}/locations/{region}/featurestores/{feature_store_id}\"\n",
    "    print(\"Feature Store: \\t\", feature_store_resource_path)\n",
    "\n",
    "    ## Run batch job request\n",
    "    try:\n",
    "        ff_feature_store = Featurestore(feature_store_resource_path)\n",
    "        ff_feature_store.batch_serve_to_gcs(\n",
    "            gcs_destination_output_uri_prefix = export_uri,\n",
    "            gcs_destination_type = 'csv',\n",
    "            serving_feature_ids = serving_feature_ids,\n",
    "            read_instances_uri = read_instances_uri,\n",
    "            pass_through_fields = ['tx_fraud','tx_amount']\n",
    "        )\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    #Store metadata\n",
    "    snapshot_pattern = f'{export_uri_path}/*.csv'\n",
    "    snapshot_files = glob.glob(snapshot_pattern)\n",
    "    snapshot_files_fmt = [p.replace('/gcs/', 'gs://') for p in snapshot_files]\n",
    "    snapshot_files_string = json.dumps(snapshot_files_fmt)\n",
    "\n",
    "    component_outputs = NamedTuple(\"Outputs\",\n",
    "                                [(\"snapshot_uri_paths\", str),],)\n",
    "\n",
    "    return component_outputs(snapshot_files_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2342064c-201f-4c41-bcdc-25ae01363325",
   "metadata": {},
   "source": [
    "#### Define an evaluate custom component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c5b40-1dcb-4537-86df-7824083867f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile deploy_kfp_pipeline/pipeline/components/evaluate_model.py\n",
    "\n",
    "from kfp.v2.dsl import Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, Metrics, ClassificationMetrics, Condition, component\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "@component(\n",
    "output_component_file='./pipelines/components/evaluate.yaml')\n",
    "def evaluate_model(\n",
    "    model_in: Input[Artifact],\n",
    "    metrics_uri: str,\n",
    "    meta_metrics: Output[Metrics],\n",
    "    graph_metrics: Output[ClassificationMetrics],\n",
    "    model_out: Output[Model]) -> NamedTuple(\"Outputs\",\n",
    "                                            [(\"metrics_thr\", float),],):\n",
    "\n",
    "    # Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "    import json\n",
    "\n",
    "    # Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "    metrics_path = metrics_uri.replace('gs://', '/gcs/')\n",
    "    labels = ['not fraud', 'fraud']\n",
    "\n",
    "    # Main -------------------------------------------------------------------------------------------------------------------------------\n",
    "    with open(metrics_path, mode='r') as json_file:\n",
    "        metrics = json.load(json_file)\n",
    "\n",
    "    ## metrics\n",
    "    fpr = metrics['fpr']\n",
    "    tpr = metrics['tpr']\n",
    "    thrs = metrics['thrs']\n",
    "    c_matrix = metrics['confusion_matrix']\n",
    "    avg_precision_score = metrics['avg_precision_score']\n",
    "    f1 = metrics['f1_score']\n",
    "    lg_loss = metrics['log_loss']\n",
    "    prec_score = metrics['precision_score']\n",
    "    rec_score = metrics['recall_score']\n",
    "\n",
    "    meta_metrics.log_metric('avg_precision_score', avg_precision_score)\n",
    "    meta_metrics.log_metric('f1_score', f1)\n",
    "    meta_metrics.log_metric('log_loss', lg_loss)\n",
    "    meta_metrics.log_metric('precision_score', prec_score)\n",
    "    meta_metrics.log_metric('recall_score', rec_score)\n",
    "    graph_metrics.log_roc_curve(fpr, tpr, thrs)\n",
    "    graph_metrics.log_confusion_matrix(labels, c_matrix)\n",
    "\n",
    "    ## model metadata\n",
    "    model_framework = 'xgb.dask'\n",
    "    model_type = 'DaskXGBClassifier'\n",
    "    model_user = 'inardini' \n",
    "    model_function = 'classification'\n",
    "    model_out.metadata[\"framework\"] = model_framework\n",
    "    model_out.metadata[\"type\"] = model_type\n",
    "    model_out.metadata[\"model function\"] = model_function\n",
    "    model_out.metadata[\"modified by\"] = model_user\n",
    "\n",
    "    component_outputs = NamedTuple(\"Outputs\",\n",
    "                                [(\"metrics_thr\", float),],)\n",
    "\n",
    "    return component_outputs(float(avg_precision_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489adf46-189f-4550-aad6-5d2cdc0b7666",
   "metadata": {},
   "source": [
    "### Define Custom Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb3119-a38e-4887-b68c-38b8001b598f",
   "metadata": {},
   "source": [
    "#### Define feature store component\n",
    "\n",
    "Notice that the component assumes that containes the entities-timestamps \"query\" is already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d37402-e689-4923-bc26-8af98271c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile deploy_kfp_pipeline/pipeline/kfp_pipeline.py\n",
    "\n",
    "#General\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#Vertex Pipelines\n",
    "from typing import NamedTuple\n",
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, Metrics, ClassificationMetrics, Condition, component\n",
    "from kfp.v2 import compiler\n",
    "import google_cloud_pipeline_components\n",
    "from google_cloud_pipeline_components import aiplatform as vertex_ai_components\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from pipeline.components.batch_serve import features_to_gcs\n",
    "from pipeline.components.evaluate_model import evaluate_model\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\", \"\") # These variables would be passed from Cloud Build in CI/CD in case the deployment is supposed to be on different project, cusch as test, eval, prod.\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "\n",
    "client = storage.Client()\n",
    "bucket =  client.get_bucket(BUCKET_NAME)\n",
    "blob = bucket.get_blob('config/notebook_env.py')\n",
    "config = blob.download_as_string()\n",
    "exec(config)\n",
    "\n",
    "\n",
    "\n",
    "# TODO to load it from config file\n",
    "IMAGE_REPOSITORY = f'fraudfinder-{ID}'\n",
    "IMAGE_NAME='dask-xgb-classificator'\n",
    "IMAGE_TAG='v1'\n",
    "IMAGE_URI=f\"us-central1-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "#Components\n",
    "BASE_IMAGE='python:3.7'\n",
    "COMPONENTS_DIR=os.path.join(os.curdir, 'pipelines', 'components')\n",
    "INGEST_FEATURE_STORE=f\"{COMPONENTS_DIR}/ingest_feature_store_{TIMESTAMP}.yaml\"\n",
    "EVALUATE=f\"{COMPONENTS_DIR}/evaluate_{TIMESTAMP}.yaml\"\n",
    "\n",
    "#Pipeline\n",
    "PIPELINE_NAME = f'fraud-finder-xgb-pipeline2-{ID}'\n",
    "PIPELINE_DIR=os.path.join(os.curdir, 'pipelines')\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipelines\"\n",
    "PIPELINE_PACKAGE_PATH = f\"{PIPELINE_DIR}/pipeline_{TIMESTAMP}.json\"\n",
    "\n",
    "#Feature Store component\n",
    "START_DATE_TRAIN = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE_TRAIN = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "BQ_DATASET = \"tx\"\n",
    "READ_INSTANCES_TABLE = f\"ground_truth\"\n",
    "READ_INSTANCES_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
    "\n",
    "#Dataset component\n",
    "DATASET_NAME = f'fraud_finder_dataset_{END_DATE_TRAIN}'\n",
    "\n",
    "#Training component\n",
    "JOB_NAME = f'fraudfinder-train-xgb-{TIMESTAMP}'\n",
    "MODEL_NAME = f'fraudfinder-xgb-{ID}'\n",
    "TRAIN_MACHINE_TYPE = 'n2-standard-4'\n",
    "CONTAINER_URI = 'us-docker.pkg.dev/vertex-ai/training/xgboost-cpu.1-1:latest'\n",
    "MODEL_SERVING_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-1:latest'\n",
    "PYTHON_MODULE = 'trainer.train_model'\n",
    "ARGS=[\"--bucket\", f\"gs://{BUCKET_NAME}\"]\n",
    "\n",
    "#Evaluation component\n",
    "METRICS_URI = f\"gs://{BUCKET_NAME}/deliverables/metrics.json\"\n",
    "AVG_PR_THRESHOLD = 0.8\n",
    "AVG_PR_CONDITION = 'avg_pr_condition'\n",
    "\n",
    "#endpoint\n",
    "ENDPOINT_NAME = 'fraudfinder_xgb_prediction'\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "pipeline_root=PIPELINE_ROOT,\n",
    "name=PIPELINE_NAME,)\n",
    "def pipeline(project_id:str = PROJECT_ID, \n",
    "             region:str = REGION, \n",
    "             bucket_name:str = f\"gs://{BUCKET_NAME}\",\n",
    "             feature_store_id:str = FEATURESTORE_ID, \n",
    "             read_instances_uri:str = READ_INSTANCES_URI,\n",
    "             replica_count:int = 1,\n",
    "             machine_type:str = \"n1-standard-4\",\n",
    "             train_split:float = 0.8,\n",
    "             test_split:float = 0.1,\n",
    "             val_split:float = 0.1,\n",
    "             metrics_uri: str = METRICS_URI, \n",
    "             thold: float = AVG_PR_THRESHOLD,\n",
    "            ):\n",
    "\n",
    "    #Export data from featurestore\n",
    "    features_to_gcs_op = features_to_gcs(project_id=project_id, region=region, bucket_name=bucket_name, \n",
    "                                             feature_store_id=feature_store_id, read_instances_uri=read_instances_uri)\n",
    "\n",
    "    #create dataset \n",
    "    dataset_create_op = vertex_ai_components.TabularDatasetCreateOp(project=project_id,\n",
    "                                                       display_name=DATASET_NAME,\n",
    "                                                       gcs_source=features_to_gcs_op.outputs['snapshot_uri_paths']).after(features_to_gcs_op)\n",
    "\n",
    "    #custom training job component - script\n",
    "    train_model_op = vertex_ai_components.CustomContainerTrainingJobRunOp(\n",
    "        display_name=JOB_NAME,\n",
    "        model_display_name=MODEL_NAME,\n",
    "        container_uri=IMAGE_URI,\n",
    "        staging_bucket=bucket_name,\n",
    "        dataset=dataset_create_op.outputs['dataset'],\n",
    "        base_output_dir=bucket_name,\n",
    "        args = ARGS,\n",
    "        replica_count= replica_count,\n",
    "        machine_type= machine_type,\n",
    "        training_fraction_split=train_split,\n",
    "        validation_fraction_split=val_split,\n",
    "        test_fraction_split=test_split,\n",
    "        model_serving_container_image_uri=MODEL_SERVING_IMAGE_URI,\n",
    "        project=project_id,\n",
    "        location=region).after(dataset_create_op)\n",
    "\n",
    "    #evaluate component\n",
    "    evaluate_model_op = evaluate_model(model_in=train_model_op.outputs[\"model\"], \n",
    "                                       metrics_uri=metrics_uri).after(train_model_op)\n",
    "\n",
    "    #if threshold\n",
    "    with Condition(evaluate_model_op.outputs['metrics_thr'] < thold, name=AVG_PR_CONDITION):\n",
    "\n",
    "        #create endpoint\n",
    "        create_endpoint_op = vertex_ai_components.EndpointCreateOp(\n",
    "            display_name=ENDPOINT_NAME,\n",
    "            project=project_id).after(evaluate_model_op)\n",
    "\n",
    "        #deploy th model\n",
    "        custom_model_deploy_op = vertex_ai_components.ModelDeployOp(\n",
    "            model=train_model_op.outputs[\"model\"],\n",
    "            endpoint=create_endpoint_op.outputs[\"endpoint\"],\n",
    "            deployed_model_display_name=MODEL_NAME,\n",
    "            dedicated_resources_machine_type=machine_type,\n",
    "        dedicated_resources_min_replica_count=replica_count\n",
    "        ).after(create_endpoint_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377ccc3-8f1f-47ce-8b46-57c29270c69d",
   "metadata": {},
   "source": [
    "## Compile your pipeline into a JSON file\n",
    "\n",
    "Please check the `deploy_kfp_pipeline` directory. We already have added the code for building the pipline and its's components in the `pipeine` directory. Please feel free to change it.\n",
    "\n",
    "After the workflow of your pipeline is defined, you can proceed to compile the pipeline into a JSON format. The JSON file will include all the information for executing your pipeline on Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b308e0-8ce8-4205-8152-7a11ddeb3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python deploy_kfp_pipeline/pipeline_compile.py  --pipeline-name=$PIPELINE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eca42c-a9a9-4f80-be42-da894065e298",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Submit your pipeline run\n",
    "Once the workflow of your pipeline is compiled into the JSON format, you can use the Vertex AI Python client to submit and run your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db290d4f-694d-4f67-81a0-e92277e28010",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ubla set on gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c72bc-0ab8-47d9-ab6a-447051820463",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines_file_location = os.path.join('./pipelines/', f'{PIPELINE_NAME}.json')\n",
    "!python ./deploy_kfp_pipeline/pipeline_run.py --pipelines-file-location=$pipelines_file_location"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
