{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FraudFinder - Model Training XGBoost and model formalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\"\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows how to get your training dataset from Vertex AI Feature Store, train your model using Vertex AI managed training pipeline, and deploy it as a Vertex AI endpoint. You will learn how to use your own custom code for ML training on Vertex AI.\n",
    "\n",
    "### Objective\n",
    "\n",
    "In the following notebook, you will learn how to:\n",
    "\n",
    "* build a container to run your own custom code on Vertex AI\n",
    "* use Vertex AI to train your model at scale\n",
    "* use Vertex AI to create an endpoint\n",
    "\n",
    "This tutorial uses the following Google Cloud data analytics and services:\n",
    "\n",
    "- [BigQuery](https://cloud.google.com/bigquery/)\n",
    "- [BigQuery ML](https://cloud.google.com/bigquery-ml/)\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* BigQuery\n",
    "* BigQuery ML\n",
    "* Vertex AI\n",
    "\n",
    "Learn about [BigQuery Pricing](https://cloud.google.com/bigquery/pricing), [BigQuery ML pricing](https://cloud.google.com/bigquery-ml/pricing), [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BUCKET_NAME          = \"ff04-dryrun-fraudfinder\"\n",
      "PROJECT              = \"ff04-dryrun\"\n",
      "REGION               = \"us-central1\"\n",
      "ID                   = \"698au\"\n",
      "FEATURESTORE_ID      = \"fraudfinder_698au\"\n",
      "MODEL_NAME           = \"fraudfinder_logreg_model\"\n",
      "ENDPOINT_NAME        = \"fraudfinder_logreg_endpoint\"\n",
      "TRAINING_DS_SIZE     = \"1000\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "#General\n",
    "import os\n",
    "import sys\n",
    "from typing import Union, List\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#Data Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Model Training with Vertex AI\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform_v1 import ModelServiceClient\n",
    "from google.cloud.aiplatform_v1.types import ListModelEvaluationsRequest\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud import storage\n",
    "\n",
    "#Model Deployment and Evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "#Feature Store\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import Featurestore, EntityType, Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "DATA_DIR = os.path.join(os.pardir, \"data\")\n",
    "TRAIN_DATA_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "DATA_URI = f\"gs://{BUCKET_NAME}/data\"\n",
    "TRAIN_DATA_URI = f\"{DATA_URI}/train\"\n",
    "\n",
    "#Feature Store\n",
    "START_DATE_TRAIN = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\") \n",
    "CUSTOMER_ENTITY = \"customer\"\n",
    "TERMINAL_ENTITY = \"terminal\"\n",
    "SERVING_FEATURE_IDS = {CUSTOMER_ENTITY: [\"*\"], TERMINAL_ENTITY: [\"*\"]}\n",
    "READ_INSTANCES_TABLE = f\"ground_truth_{ID}\"\n",
    "READ_INSTANCES_URI = f\"bq://{PROJECT_ID}.tx.{READ_INSTANCES_TABLE}\"\n",
    "\n",
    "# Training\n",
    "EXPERIMENT_NAME=f\"fraudfinder-xgb-experiment-{ID}\"\n",
    "TARGET = \"tx_fraud\"\n",
    "\n",
    "## Custom Training\n",
    "DATASET_NAME=f\"sample_train-{ID}\"\n",
    "TRAIN_JOB_NAME=f\"fraudfinder_xgb_train_frmlz-{ID}\"\n",
    "MODEL_NAME=f\"fraudfinder_xgb_model_frmlz-{ID}\"\n",
    "DEPLOYED_NAME = f\"fraudfinder_xgb_prediction_frmlz-{ID}\"\n",
    "MODEL_SERVING_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-1:latest\"\n",
    "IMAGE_REPOSITORY = f\"fraudfinder-{ID}\"\n",
    "IMAGE_NAME=\"dask-xgb-classificator\"\n",
    "IMAGE_TAG=\"v1\"\n",
    "IMAGE_URI=f\"us-central1-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "TRAIN_COMPUTE=\"e2-standard-4\"\n",
    "DEPLOY_COMPUTE=\"n1-standard-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Initialize Vertex AI SDK and BigQuery Client for Python\n",
    "Next you will initialize the Vertex AI SDK and BigQuery Client for Python for your project and corresponding bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME, experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "You will now run some helper functions that we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gcs_dataset(client,\n",
    "                       display_name: str, \n",
    "                       gcs_source: Union[str, List[str]]):\n",
    "    \n",
    "    dataset = client.TabularDataset.create(\n",
    "        display_name=display_name, gcs_source=gcs_source,\n",
    "    )\n",
    "\n",
    "    dataset.wait()\n",
    "    return dataset\n",
    "    \n",
    "def get_evaluation_metrics(client, model_resource_name):\n",
    "    model_evalution_request = ListModelEvaluationsRequest(parent=model_resource_name)\n",
    "    model_evaluation_list = client.list_model_evaluations(request=model_evalution_request)\n",
    "    metrics_strlist = []\n",
    "    for evaluation in model_evaluation_list:\n",
    "        metrics = MessageToDict(evaluation._pb.metrics)\n",
    "    return metrics\n",
    "\n",
    "def gcs_list(gcs_uri):\n",
    "    obj_list=[]\n",
    "    storage_client = storage.Client()\n",
    "    bucket, key = gcs_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "    for blob in storage_client.list_blobs(bucket, prefix=key):\n",
    "        obj_list.append(\"gs://\"+bucket+\"/\"+str(blob.name))\n",
    "    return obj_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also using the BigQuery helper function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to use BigQuery client to run query/job, return job ID or result as DF\n",
    "def run_bq_query(sql: str) -> Union[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "    Args:\n",
    "        sql: SQL query, as a string, to execute in BigQuery\n",
    "    Returns:\n",
    "        df: DataFrame of results from query,  or error, if any\n",
    "    \"\"\"\n",
    "\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Try dry run before executing query to catch any errors\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    # If dry run succeeds without errors, proceed to run query\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    job_id = client_result.job_id\n",
    "\n",
    "    # Wait for query/job to finish running. then get & return data frame\n",
    "    df = client_result.result().to_arrow().to_pandas()\n",
    "    print(f\"Finished job_id: {job_id}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fetching feature values for model training\n",
    "\n",
    "To fetch training data, we have to specify the following inputs to batch serving:\n",
    "\n",
    "- a file containing a \"query\", with the entities and timestamps for each label\n",
    "- a list of feature values to fetch\n",
    "- the destination location and format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read-instance list\n",
    "\n",
    "In our case, we need a csv file with content formatted like the table below:\n",
    "\n",
    "|event|customer                     |terminal|timestamp                                    |\n",
    "|-----|-----------------------------|--------|---------------------------------------------|\n",
    "|xxx57538|xxx3859                         |xxx8811    |2021-07-07 00:01:10 UTC                      |\n",
    "|xxx57539|xxx4165                         |xxx8810    |2021-07-07 00:01:55 UTC                      |\n",
    "|xxx57540|xxx2289                         |xxx2081    |2021-07-07 00:02:12 UTC                      |\n",
    "|xxx57541|xxx3227                         |xxx3011    |2021-07-07 00:03:23 UTC                      |\n",
    "|xxx57542|xxx2819                         |xxx6263    |2021-07-07 00:05:30 UTC                      |\n",
    "\n",
    "where the column names are the name of entities in Feature Store and the timestamps represents the time an event occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE OR REPLACE TABLE ff04-dryrun.tx.ground_truth_698au as (\n",
      "    SELECT\n",
      "        raw_tx.TX_TS AS timestamp,\n",
      "        raw_tx.CUSTOMER_ID AS customer,\n",
      "        raw_tx.TERMINAL_ID AS terminal,\n",
      "        raw_tx.TX_AMOUNT AS tx_amount,\n",
      "        raw_lb.TX_FRAUD AS tx_fraud,\n",
      "    FROM \n",
      "        tx.tx as raw_tx\n",
      "    LEFT JOIN \n",
      "        tx.txlabels as raw_lb\n",
      "    ON raw_tx.TX_ID = raw_lb.TX_ID\n",
      "    WHERE\n",
      "        DATE(raw_tx.TX_TS) = \"2023-01-24\"\n",
      "    LIMIT 1000\n",
      ");\n",
      "\n",
      "Finished job_id: 847ff94c-9315-4224-b99a-28a43069fad9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {PROJECT_ID}.tx.{READ_INSTANCES_TABLE} as (\n",
    "    SELECT\n",
    "        raw_tx.TX_TS AS timestamp,\n",
    "        raw_tx.CUSTOMER_ID AS customer,\n",
    "        raw_tx.TERMINAL_ID AS terminal,\n",
    "        raw_tx.TX_AMOUNT AS tx_amount,\n",
    "        raw_lb.TX_FRAUD AS tx_fraud,\n",
    "    FROM \n",
    "        tx.tx as raw_tx\n",
    "    LEFT JOIN \n",
    "        tx.txlabels as raw_lb\n",
    "    ON raw_tx.TX_ID = raw_lb.TX_ID\n",
    "    WHERE\n",
    "        DATE(raw_tx.TX_TS) = \"{START_DATE_TRAIN}\"\n",
    "    LIMIT {TRAINING_DS_SIZE}\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(sql_query)\n",
    "\n",
    "run_bq_query(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Feature Store ID\n",
    "Initiate the feature store you created in the `02_feature_engineering_batch.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ff_feature_store = Featurestore(FEATURESTORE_ID)\n",
    "except NameError:\n",
    "    print(f\"\"\"The feature store {FEATURESTORE_ID} does not exist!\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch a sample of data and dump it into a bucket \n",
    "In this section, we will use the Batch Serving of the Vetex AI Feature Store to prepare a dataset for training. You will do this by calling the BatchReadFeatureValues API. Batch Serving is used to fetch large feature values with high throughput, typically for training a model or batch prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Uniform bucket-level access for gs://ff04-dryrun-fraudfinder...\n"
     ]
    }
   ],
   "source": [
    "!gsutil uniformbucketlevelaccess set on gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving Featurestore feature values: projects/1003399910665/locations/us-central1/featurestores/fraudfinder_698au\n",
      "Serve Featurestore feature values backing LRO: projects/1003399910665/locations/us-central1/featurestores/fraudfinder_698au/operations/5462959600330342400\n",
      "Featurestore feature values served. Resource name: projects/1003399910665/locations/us-central1/featurestores/fraudfinder_698au\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.featurestore.featurestore.Featurestore object at 0x7fce263f5910> \n",
       "resource name: projects/1003399910665/locations/us-central1/featurestores/fraudfinder_698au"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_feature_store.batch_serve_to_gcs(\n",
    "    gcs_destination_output_uri_prefix = TRAIN_DATA_URI,\n",
    "    gcs_destination_type = \"csv\",\n",
    "    serving_feature_ids = SERVING_FEATURE_IDS, \n",
    "    read_instances_uri = READ_INSTANCES_URI,\n",
    "    pass_through_fields=[\"tx_amount\", \"tx_fraud\"],  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabling Uniform bucket-level access for gs://ff04-dryrun-fraudfinder...\n"
     ]
    }
   ],
   "source": [
    "!gsutil uniformbucketlevelaccess set off gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will create a copy of the training data in your local notebook instance. You will need it later for testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ff04-dryrun-fraudfinder/data/train/000000000000.csv\n",
      "Copying gs://ff04-dryrun-fraudfinder/data/train/000000000000.csv...\n",
      "/ [1 files][169.9 KiB/169.9 KiB]                                                \n",
      "Operation completed over 1 objects/169.9 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $TRAIN_DATA_URI\n",
    "!sudo gsutil cp -r $TRAIN_DATA_URI $TRAIN_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the features into cloud storage will generate a csv file. Let's list the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gs://ff04-dryrun-fraudfinder/data/train/000000000000.csv']\n"
     ]
    }
   ],
   "source": [
    "obj_list = gcs_list(TRAIN_DATA_URI)\n",
    "print(obj_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Builing a custom fraud detection model\n",
    "\n",
    "### Fixing an imbalanced dataset\n",
    "In the real world, you will need to deal with an imbalance in the dataset. For example, you might randomly delete some of the non-fraudulent transactions to approximately match the number of fraudulent transactions. This technique is called undersampling.\n",
    "\n",
    "For this workshop, we will skip the data balance process because our sample data is small, and the further reduction will compromise the quality of our results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Builing a Vertex AI dataset\n",
    "In this section, you will create a managed [Vertex AI dataset](https://cloud.google.com/vertex-ai/docs/training/using-managed-datasets). Vertex AI datasets can be used to train AutoML models or custom-trained models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TabularDataset\n",
      "Create TabularDataset backing LRO: projects/1003399910665/locations/us-central1/datasets/5573162757429133312/operations/2249641266201493504\n",
      "TabularDataset created. Resource name: projects/1003399910665/locations/us-central1/datasets/5573162757429133312\n",
      "To use this TabularDataset in another session:\n",
      "ds = aiplatform.TabularDataset('projects/1003399910665/locations/us-central1/datasets/5573162757429133312')\n",
      "Dataset: sample_train-698au\n",
      "Name: \t projects/1003399910665/locations/us-central1/datasets/5573162757429133312\n"
     ]
    }
   ],
   "source": [
    "dataset = create_gcs_dataset(client=vertex_ai, display_name=DATASET_NAME, gcs_source=obj_list[0]) #obj_list\n",
    "\n",
    "print(\"Dataset:\", f\"{dataset.display_name}\")\n",
    "print(\"Name: \\t\", f\"{dataset.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a custom model\n",
    "In this section, you will use the xgboost algorithm. Specifically, you will do custom training using a xgboost container.\n",
    "\n",
    "#### Create the training application\n",
    "To perform custom training, you can use either a pre-built container or buy your container. In this section, we will build a container for xgboost and use it to train a model with the Vertex AI Managed Training service.\n",
    "\n",
    "\n",
    "The first step is to write your training code. After, you have to write a Dockerfile and build a container image based on it. The following cell writes our code into `train_gb.py`, the module for training an XGBClassifier. We will copy this code into our container to run through the Vertex Training service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p -m 777 build_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting build_training/train_xgb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_training/train_xgb.py\n",
    "\n",
    "\"\"\"\n",
    "train_gb.py is the module for training a XGBClassifier pipeline\n",
    "\"\"\"\n",
    "\n",
    "# Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import dask.dataframe as dask_df\n",
    "from dask.distributed import LocalCluster, Client\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, average_precision_score, f1_score, log_loss, precision_score, recall_score\n",
    "\n",
    "# Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "## Read environmental variables\n",
    "TRAINING_DATA_PATH = os.environ[\"AIP_TRAINING_DATA_URI\"].replace(\"gs://\", \"/gcs/\")\n",
    "TEST_DATA_PATH = os.environ[\"AIP_TEST_DATA_URI\"].replace(\"gs://\", \"/gcs/\")\n",
    "MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"].replace(\"gs://\", \"/gcs/\")\n",
    "MODEL_PATH = MODEL_DIR + \"model.bst\"\n",
    "\n",
    "\n",
    "## Training variables\n",
    "LABEL_COLUMN = \"tx_fraud\"\n",
    "UNUSED_COLUMNS = [\"timestamp\",\"entity_type_customer\",\"entity_type_terminal\"]\n",
    "DATA_SCHEMA = {\n",
    "\"timestamp\" : \"object\",\n",
    "\"tx_amount\": \"float64\",\n",
    "\"tx_fraud\": \"Int64\",\n",
    "\"entity_type_customer\": \"Int64\",\n",
    "\"customer_id_nb_tx_1day_window\": \"Int64\",\n",
    "\"customer_id_nb_tx_7day_window\": \"Int64\",\n",
    "\"customer_id_nb_tx_14day_window\": \"Int64\",\n",
    "\"customer_id_avg_amount_1day_window\": \"float64\",\n",
    "\"customer_id_avg_amount_7day_window\": \"float64\",\n",
    "\"customer_id_avg_amount_14day_window\": \"float64\",\n",
    "\"customer_id_nb_tx_15min_window\": \"Int64\",\n",
    "\"customer_id_avg_amount_15min_window\": \"float64\",\n",
    "\"customer_id_nb_tx_30min_window\": \"Int64\",\n",
    "\"customer_id_avg_amount_30min_window\": \"float64\",\n",
    "\"customer_id_nb_tx_60min_window\": \"Int64\",\n",
    "\"customer_id_avg_amount_60min_window\": \"float64\",\n",
    "\"entity_type_terminal\": \"Int64\",\n",
    "\"terminal_id_nb_tx_1day_window\": \"Int64\",\n",
    "\"terminal_id_nb_tx_7day_window\": \"Int64\",\n",
    "\"terminal_id_nb_tx_14day_window\": \"Int64\",\n",
    "\"terminal_id_risk_1day_window\": \"float64\",\n",
    "\"terminal_id_risk_7day_window\": \"float64\",\n",
    "\"terminal_id_risk_14day_window\": \"float64\",\n",
    "\"terminal_id_nb_tx_15min_window\": \"Int64\",\n",
    "\"terminal_id_avg_amount_15min_window\": \"float64\",\n",
    "\"terminal_id_nb_tx_30min_window\": \"Int64\",\n",
    "\"terminal_id_avg_amount_30min_window\": \"float64\",\n",
    "\"terminal_id_nb_tx_60min_window\": \"Int64\",\n",
    "\"terminal_id_avg_amount_60min_window\": \"float64\"\n",
    "}\n",
    "\n",
    "# Helpers -----------------------------------------------------------------------------------------------------------------------------\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data files arguments\n",
    "    parser.add_argument(\"--bucket\", dest=\"bucket\", type=str,\n",
    "                        required=True, help=\"Bucket uri\")\n",
    "    parser.add_argument(\"--max_depth\", dest=\"max_depth\",\n",
    "                        default=6, type=int,\n",
    "                        help=\"max_depth value.\")\n",
    "    parser.add_argument(\"--eta\", dest=\"eta\",\n",
    "                        default=0.4, type=float,\n",
    "                        help=\"eta.\")\n",
    "    parser.add_argument(\"--gamma\", dest=\"gamma\",\n",
    "                        default=0.0, type=float,\n",
    "                        help=\"eta value\")\n",
    "    parser.add_argument(\"-v\", \"--verbose\", \n",
    "                        help=\"increase output verbosity\", \n",
    "                        action=\"store_true\")\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def set_logging():\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "def resample(df, replace, frac=1, random_state = 8):\n",
    "    shuffled_df = df.sample(frac=frac, replace=replace, random_state=random_state)\n",
    "    return shuffled_df\n",
    "\n",
    "def preprocess(df):\n",
    "    \n",
    "    df = df.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "    # Drop rows with NaN\"s\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = df.select_dtypes([\"float32\", \"float64\"]).columns\n",
    "    numeric_format = {col:\"float32\" for col in numeric_columns}\n",
    "    df.astype(numeric_format)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate_model(model, x_true, y_true):\n",
    "    \n",
    "    y_true = y_true.compute()\n",
    "    \n",
    "    #calculate metrics\n",
    "    metrics={}\n",
    "    \n",
    "    y_score =  model.predict_proba(x_true)[:, 1]\n",
    "    y_score = y_score.compute()\n",
    "    fpr, tpr, thr = roc_curve(\n",
    "         y_true=y_true, y_score=y_score, pos_label=True\n",
    "    )\n",
    "    fpr_list = fpr.tolist()[::1000]\n",
    "    tpr_list = tpr.tolist()[::1000]\n",
    "    thr_list = thr.tolist()[::1000]\n",
    "\n",
    "    y_pred = model.predict(x_true)\n",
    "    y_pred.compute()\n",
    "    c_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    avg_precision_score = round(average_precision_score(y_true, y_score), 3)\n",
    "    f1 = round(f1_score(y_true, y_pred), 3)\n",
    "    lg_loss = round(log_loss(y_true, y_pred), 3)\n",
    "    prec_score = round(precision_score(y_true, y_pred), 3)\n",
    "    rec_score = round(recall_score(y_true, y_pred), 3)\n",
    "    \n",
    "    \n",
    "    metrics[\"fpr\"] = [round(f, 3) for f in fpr_list]\n",
    "    metrics[\"tpr\"] = [round(f, 3) for f in tpr_list]\n",
    "    metrics[\"thrs\"] = [round(f, 3) for f in thr_list]\n",
    "    metrics[\"confusion_matrix\"] = c_matrix.tolist()\n",
    "    metrics[\"avg_precision_score\"] = avg_precision_score\n",
    "    metrics[\"f1_score\"] = f1\n",
    "    metrics[\"log_loss\"] = lg_loss\n",
    "    metrics[\"precision_score\"] = prec_score\n",
    "    metrics[\"recall_score\"] = rec_score\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    if args.verbose:\n",
    "        set_logging()\n",
    "        \n",
    "    #variables\n",
    "    bucket = args.bucket.replace(\"gs://\", \"/gcs/\")\n",
    "    deliverable_uri = (Path(bucket)/\"deliverables\")\n",
    "    metrics_uri = (deliverable_uri/\"metrics.json\")\n",
    "\n",
    "    #read data\n",
    "    train_df = dask_df.read_csv(TRAINING_DATA_PATH, dtype=DATA_SCHEMA)\n",
    "    test_df = dask_df.read_csv(TEST_DATA_PATH, dtype=DATA_SCHEMA)\n",
    "    \n",
    "    #preprocessing\n",
    "    preprocessed_train_df = preprocess(train_df)\n",
    "    preprocessed_test_df = preprocess(test_df)\n",
    "    \n",
    "    #downsampling\n",
    "    train_nfraud_df = preprocessed_train_df[preprocessed_train_df[LABEL_COLUMN]==0]\n",
    "    train_fraud_df = preprocessed_train_df[preprocessed_train_df[LABEL_COLUMN]==1]\n",
    "    train_nfraud_downsample = resample(train_nfraud_df,\n",
    "                          replace=True, \n",
    "                          frac=len(train_fraud_df)/len(train_df))\n",
    "    ds_preprocessed_train_df = dask_df.multi.concat([train_nfraud_downsample, train_fraud_df])\n",
    "    \n",
    "    #target, features split\n",
    "    x_train = ds_preprocessed_train_df[ds_preprocessed_train_df.columns.difference([LABEL_COLUMN])]\n",
    "    y_train = ds_preprocessed_train_df.loc[:, LABEL_COLUMN].astype(int)\n",
    "    x_true = preprocessed_test_df[preprocessed_test_df.columns.difference([LABEL_COLUMN])]\n",
    "    y_true = preprocessed_test_df.loc[:, LABEL_COLUMN].astype(int)\n",
    "    \n",
    "    #train model\n",
    "    cluster =  LocalCluster()\n",
    "    client = Client(cluster)\n",
    "    model = xgb.dask.DaskXGBClassifier(objective=\"reg:logistic\", eval_metric=\"logloss\")\n",
    "    model.client = client  # assign the client\n",
    "    model.fit(x_train, y_train, eval_set=[(x_true, y_true)])\n",
    "    if not Path(MODEL_DIR).exists():\n",
    "        Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    model.save_model(MODEL_PATH)\n",
    "    \n",
    "    #generate metrics\n",
    "    metrics = evaluate_model(model, x_true, y_true)\n",
    "    if not Path(deliverable_uri).exists():\n",
    "        Path(deliverable_uri).mkdir(parents=True, exist_ok=True)\n",
    "    with open(metrics_uri, \"w\") as file:\n",
    "        json.dump(metrics, file, sort_keys = True, indent = 4)\n",
    "    file.close()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a custom image for dask model training\n",
    "\n",
    "Now you will build a custom container. By running your machine learning (ML) training job in a custom container, you can use any ML framework, non-ML dependencies, libraries, and binaries that are not otherwise supported on Vertex AI. Finally, you will package your training code into a Docker container image, push the container image to Container Registry, and create a custom job on Vertex AI.\n",
    "\n",
    "For the ML framework, we will use xgboost. You will also use dask and scikit libraries. Dask is an open-source library for parallel computing written in Python. You will use Dask to speed up pre-processing of our dataset.\n",
    "\n",
    "Let's first check if the repository already exists. If it already exists, then you can skip the cell that creates the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the repository already exists\n",
    "!gcloud artifacts repositories describe $IMAGE_REPOSITORY --location=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [fraudfinder-698au]\n",
      "Waiting for operation [projects/ff04-dryrun/locations/us-central1/operations/f9\n",
      "1d24f4-2c41-43ca-ba20-5c94c74cbebb] to complete...done.                        \n",
      "Created repository [fraudfinder-698au].\n",
      "Listing items under project ff04-dryrun, across all locations.\n",
      "\n",
      "                                                                                ARTIFACT_REGISTRY\n",
      "REPOSITORY         FORMAT  MODE                 DESCRIPTION                           LOCATION     LABELS  ENCRYPTION          CREATE_TIME          UPDATE_TIME          SIZE (MB)\n",
      "fraudfinder-698au  DOCKER  STANDARD_REPOSITORY  Fraud Finder Docker Image repository  us-central1          Google-managed key  2023-01-25T06:48:15  2023-01-25T06:48:15  0\n"
     ]
    }
   ],
   "source": [
    "# Create image repositorie\n",
    "!gcloud artifacts repositories create $IMAGE_REPOSITORY \\\n",
    "    --repository-format=docker \\\n",
    "    --location=us-central1 \\\n",
    "    --description=\"Fraud Finder Docker Image repository\"\n",
    "\n",
    "# List repositories under the project\n",
    "!gcloud artifacts repositories list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker us-central1-docker.pkg.dev -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to write your docker file in order to create your container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting build_training/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_training/Dockerfile\n",
    "# Specifies base image and tag\n",
    "FROM python:3.7\n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages\n",
    "RUN pip install gcsfs numpy pandas scikit-learn dask distributed xgboost --upgrade\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY ./train_xgb.py /root/train_xgb.py\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python3\", \"train_xgb.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will have to build the docker container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  10.24kB\n",
      "Step 1/5 : FROM python:3.7\n",
      "3.7: Pulling from library/python\n",
      "\n",
      "\u001b[1Bf03cda1f: Pulling fs layer \n",
      "\u001b[1Bf75f014e: Pulling fs layer \n",
      "\u001b[1B1d0e6b05: Pulling fs layer \n",
      "\u001b[1B50679dbd: Pulling fs layer \n",
      "\u001b[1B2ee9da04: Pulling fs layer \n",
      "\u001b[1B27d5e312: Pulling fs layer \n",
      "\u001b[1Bc5733757: Waiting fs layer \n",
      "\u001b[1Bd6676584: Pulling fs layer \n",
      "\u001b[1Bc4ff9b48: Pull complete 886MB/2.886MBB\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[6A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KDigest: sha256:1f28287096c1a96a450455e819e26a9ac096f448d653006c82846c4fe52e9014\n",
      "Status: Downloaded newer image for python:3.7\n",
      " ---> fd433822517a\n",
      "Step 2/5 : WORKDIR /root\n",
      " ---> Running in 111eb504dc22\n",
      "Removing intermediate container 111eb504dc22\n",
      " ---> ec9021e21193\n",
      "Step 3/5 : RUN pip install gcsfs numpy pandas scikit-learn dask distributed xgboost --upgrade\n",
      " ---> Running in 3e12360947d0\n",
      "Collecting gcsfs\n",
      "  Downloading gcsfs-2023.1.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 60.4 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 74.3 MB/s eta 0:00:00\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.8/24.8 MB 43.2 MB/s eta 0:00:00\n",
      "Collecting dask\n",
      "  Downloading dask-2022.2.0-py3-none-any.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 60.8 MB/s eta 0:00:00\n",
      "Collecting distributed\n",
      "  Downloading distributed-2022.2.0-py3-none-any.whl (837 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 837.0/837.0 KB 52.9 MB/s eta 0:00:00\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl (255.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 255.9/255.9 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting google-auth>=1.2\n",
      "  Downloading google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.8/177.8 KB 23.5 MB/s eta 0:00:00\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.7.0-py2.py3-none-any.whl (110 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.2/110.2 KB 14.8 MB/s eta 0:00:00\n",
      "Collecting decorator>4.1.2\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (948 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 948.0/948.0 KB 55.1 MB/s eta 0:00:00\n",
      "Collecting fsspec==2023.1.0\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.0/143.0 KB 21.3 MB/s eta 0:00:00\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 KB 10.0 MB/s eta 0:00:00\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 KB 28.0 MB/s eta 0:00:00\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 499.4/499.4 KB 41.4 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 KB 30.4 MB/s eta 0:00:00\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.1/38.1 MB 34.2 MB/s eta 0:00:00\n",
      "Collecting partd>=0.3.10\n",
      "  Downloading partd-1.3.0-py3-none-any.whl (18 kB)\n",
      "Collecting cloudpickle>=1.1.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting toolz>=0.8.2\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.8/55.8 KB 8.3 MB/s eta 0:00:00\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.7/42.7 KB 5.8 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.3.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.3/596.3 KB 40.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from distributed) (57.5.0)\n",
      "Collecting psutil>=5.0\n",
      "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.2/280.2 KB 31.3 MB/s eta 0:00:00\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting msgpack>=0.6.0\n",
      "  Downloading msgpack-1.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 299.8/299.8 KB 27.4 MB/s eta 0:00:00\n",
      "Collecting zict>=0.1.3\n",
      "  Downloading zict-2.2.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting tblib>=1.6.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 KB 17.7 MB/s eta 0:00:00\n",
      "Collecting click>=6.6\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 KB 14.2 MB/s eta 0:00:00\n",
      "Collecting tornado>=5\n",
      "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 424.0/424.0 KB 37.2 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.0/148.0 KB 18.3 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.0/60.0 KB 9.4 MB/s eta 0:00:00\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.8/94.8 KB 12.0 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=3.7.4\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 231.4/231.4 KB 25.7 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
      "Collecting six>=1.9.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 KB 20.8 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting locket\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting heapdict\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.3/120.3 KB 17.2 MB/s eta 0:00:00\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.7/77.7 KB 10.7 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 KB 9.7 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 KB 17.7 MB/s eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 KB 18.7 MB/s eta 0:00:00\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 223.0/223.0 KB 25.6 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 409.8/409.8 KB 37.6 MB/s eta 0:00:00\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 KB 11.0 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 KB 20.3 MB/s eta 0:00:00\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.11.0-py3-none-any.whl (6.6 kB)\n",
      "Installing collected packages: sortedcontainers, pytz, pyasn1, msgpack, heapdict, zipp, zict, urllib3, typing-extensions, tornado, toolz, threadpoolctl, tblib, six, rsa, pyyaml, pyasn1-modules, psutil, protobuf, packaging, oauthlib, numpy, multidict, MarkupSafe, locket, joblib, idna, google-crc32c, fsspec, frozenlist, decorator, cloudpickle, charset-normalizer, certifi, cachetools, attrs, asynctest, yarl, scipy, requests, python-dateutil, partd, jinja2, importlib-metadata, googleapis-common-protos, google-resumable-media, google-auth, async-timeout, aiosignal, xgboost, scikit-learn, requests-oauthlib, pandas, google-api-core, dask, click, aiohttp, google-cloud-core, google-auth-oauthlib, distributed, google-cloud-storage, gcsfs\n",
      "Successfully installed MarkupSafe-2.1.2 aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 asynctest-0.13.0 attrs-22.2.0 cachetools-5.3.0 certifi-2022.12.7 charset-normalizer-2.1.1 click-8.1.3 cloudpickle-2.2.1 dask-2022.2.0 decorator-5.1.1 distributed-2022.2.0 frozenlist-1.3.3 fsspec-2023.1.0 gcsfs-2023.1.0 google-api-core-2.11.0 google-auth-2.16.0 google-auth-oauthlib-0.8.0 google-cloud-core-2.3.2 google-cloud-storage-2.7.0 google-crc32c-1.5.0 google-resumable-media-2.4.1 googleapis-common-protos-1.58.0 heapdict-1.0.1 idna-3.4 importlib-metadata-6.0.0 jinja2-3.1.2 joblib-1.2.0 locket-1.0.0 msgpack-1.0.4 multidict-6.0.4 numpy-1.21.6 oauthlib-3.2.2 packaging-23.0 pandas-1.3.5 partd-1.3.0 protobuf-4.21.12 psutil-5.9.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 python-dateutil-2.8.2 pytz-2022.7.1 pyyaml-6.0 requests-2.28.2 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-1.0.2 scipy-1.7.3 six-1.16.0 sortedcontainers-2.4.0 tblib-1.7.0 threadpoolctl-3.1.0 toolz-0.12.0 tornado-6.2 typing-extensions-4.4.0 urllib3-1.26.14 xgboost-1.6.2 yarl-1.8.2 zict-2.2.0 zipp-3.11.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 3e12360947d0\n",
      " ---> 5d58b9791ad2\n",
      "Step 4/5 : COPY ./train_xgb.py /root/train_xgb.py\n",
      " ---> 7be50e8a9529\n",
      "Step 5/5 : ENTRYPOINT [\"python3\", \"train_xgb.py\"]\n",
      " ---> Running in 90df43c7076e\n",
      "Removing intermediate container 90df43c7076e\n",
      " ---> cf4bcf5295ad\n",
      "Successfully built cf4bcf5295ad\n",
      "Successfully tagged us-central1-docker.pkg.dev/ff04-dryrun/fraudfinder-698au/dask-xgb-classificator:v1\n",
      "The push refers to repository [us-central1-docker.pkg.dev/ff04-dryrun/fraudfinder-698au/dask-xgb-classificator]\n",
      "\n",
      "\u001b[1B3414d7c3: Preparing \n",
      "\u001b[1B885c2d7f: Preparing \n",
      "\u001b[1B0605ae0a: Preparing \n",
      "\u001b[1Bc9c9c384: Preparing \n",
      "\u001b[1Bd360187e: Preparing \n",
      "\u001b[1B62f7bb8b: Preparing \n",
      "\u001b[1B1a405763: Preparing \n",
      "\u001b[1B964da727: Preparing \n",
      "\u001b[1B33f7bad4: Preparing \n",
      "\u001b[1Bdbda4664: Preparing \n",
      "\u001b[10B85c2d7f: Pushed   1.103GB/1.093GB\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[8A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2KPushing  9.958MB/18.48MB\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[3A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2Kv1: digest: sha256:fdbc8da81e904fd433ac4e0d18a243f0c1f6293ffad8f5504cc7a4ca7ae2e82a size: 2640\n"
     ]
    }
   ],
   "source": [
    "# Build and push docker file\n",
    "!docker build -t $IMAGE_URI ./build_training/\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start a custom training job on Vertex AI\n",
    "In this section, you will create a training pipeline. This will create custom training jobs, load our dataset and upload the model to Vertex AI after the training job is successfully completed. Learn more about creating of custom jobs [here](https://cloud.google.com/vertex-ai/docs/training/create-custom-job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://ff04-dryrun-fraudfinder/aiplatform-custom-training-2023-01-25-06:51:32.715 \n",
      "No dataset split provided. The service will use a default split.\n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/6647510000019177472?project=1003399910665\n",
      "CustomContainerTrainingJob projects/1003399910665/locations/us-central1/trainingPipelines/6647510000019177472 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/1003399910665/locations/us-central1/trainingPipelines/6647510000019177472 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/1003399910665/locations/us-central1/trainingPipelines/6647510000019177472 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/1003399910665/locations/us-central1/trainingPipelines/6647510000019177472 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4420902196749533184?project=1003399910665\n",
      "CustomContainerTrainingJob projects/1003399910665/locations/us-central1/trainingPipelines/6647510000019177472 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob run completed. Resource name: projects/1003399910665/locations/us-central1/trainingPipelines/6647510000019177472\n",
      "Model available at projects/1003399910665/locations/us-central1/models/6986228713167781888\n"
     ]
    }
   ],
   "source": [
    "job = vertex_ai.CustomContainerTrainingJob(\n",
    "    display_name=TRAIN_JOB_NAME,\n",
    "    container_uri=IMAGE_URI,\n",
    "    model_serving_container_image_uri=MODEL_SERVING_IMAGE_URI,\n",
    ")\n",
    "\n",
    "parameters = {\"MAX_DEPTH\": 4, \"ETA\": 0.3, \"GAMMA\": 0.1}\n",
    "\n",
    "CMDARGS = [ f\"\"\"--bucket={BUCKET_NAME}\"\"\",\n",
    "    \"--max_depth=\" + str(parameters[\"MAX_DEPTH\"]),\n",
    "    \"--eta=\" + str(parameters[\"ETA\"]),\n",
    "    \"--gamma=\" + str(parameters[\"GAMMA\"]),\n",
    "    \"--verbose\"\n",
    "]\n",
    "\n",
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=MODEL_NAME,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE,\n",
    "    accelerator_count=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can run the model via an endpoint, you need to pre-process the data to match the format that your custom model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"tx_fraud\"\n",
    "UNUSED_COLUMNS = [\"timestamp\",\"entity_type_customer\",\"entity_type_terminal\"]\n",
    "NA_VALUES = [\"NA\", \".\"]\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
    "\n",
    "    Args:\n",
    "      df: Pandas df with raw data\n",
    "\n",
    "    Returns:\n",
    "      df with preprocessed data\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "    # Drop rows with NaN's\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = df.select_dtypes([\"int32\", \"float32\", \"float64\"]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype(\"float32\")\n",
    "\n",
    "    dummy_columns = list(df.dtypes[df.dtypes == \"category\"].index)\n",
    "    df = pd.get_dummies(df, columns=dummy_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "#test set\n",
    "train_sample_path = os.path.join(TRAIN_DATA_DIR, \"000000000000.csv\")\n",
    "df_test = pd.read_csv(train_sample_path)\n",
    "preprocessed_test_Data = preprocess(df_test)\n",
    "\n",
    "x_test = preprocessed_test_Data[preprocessed_test_Data.columns.drop(LABEL_COLUMN).to_list()].values\n",
    "y_test = preprocessed_test_Data.loc[:,LABEL_COLUMN].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will copy the model artifact to the local directory to evaluate the model localy before deploying the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://ff04-dryrun-fraudfinder/aiplatform-custom-training-2023-01-25-06:51:32.715/model/model.bst...\n",
      "/ [1 files][ 30.0 KiB/ 30.0 KiB]                                                \n",
      "Operation completed over 1 objects/30.0 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r $model.uri ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.956484, 0.978, 0.9671223458038423, None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst = xgb.Booster()  \n",
    "bst.load_model(\"./model/model.bst\") \n",
    "xgtest = xgb.DMatrix(x_test)\n",
    "y_pred_prob = bst.predict(xgtest)\n",
    "y_pred = y_pred_prob.round().astype(int)\n",
    "y_pred_prob[0:10]\n",
    "precision_recall_fscore_support(y_test.values, y_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the model\n",
    "Before you use your model to make predictions, you need to deploy it to an Endpoint. You can do this by calling the deploy function on the Model resource. This will do two things:\n",
    "\n",
    "- create an Endpoint resource\n",
    "- deploy the Model resource to the Endpoint resource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/1003399910665/locations/us-central1/endpoints/5600564786216566784/operations/8713432631384997888\n",
      "Endpoint created. Resource name: projects/1003399910665/locations/us-central1/endpoints/5600564786216566784\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/1003399910665/locations/us-central1/endpoints/5600564786216566784')\n",
      "Deploying model to Endpoint : projects/1003399910665/locations/us-central1/endpoints/5600564786216566784\n",
      "Deploy Endpoint model backing LRO: projects/1003399910665/locations/us-central1/endpoints/5600564786216566784/operations/1473896230386925568\n",
      "Endpoint model deployed. Resource name: projects/1003399910665/locations/us-central1/endpoints/5600564786216566784\n"
     ]
    }
   ],
   "source": [
    "DEPLOY_COMPUTE=\"n1-standard-4\"\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    traffic_split=TRAFFIC_SPLIT,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    accelerator_count=0,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the deployed model (Make an online prediction request)\n",
    "Send an online prediction request to your deployed model. To make sure your deployed model is working, test it out by sending a request to the endpoint.\n",
    "\n",
    "Let's first get a test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"instances\": x_test[:2].tolist()\n",
    "}\n",
    "\n",
    "# In case you want to test it in the console\n",
    "import json\n",
    "with open(\"predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[0.0277096051722765, 0.0277096051722765], deployed_model_id='7220567626395680768', model_version_id='1', model_resource_name='projects/1003399910665/locations/us-central1/models/6986228713167781888', explanations=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.predict(instances = payload[\"instances\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (DO NOT RUN) Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint resource\n",
    "#! gcloud ai endpoints delete $ENDPOINT_NAME --quiet --region $REGION_NAME\n",
    "\n",
    "# Delete model resource\n",
    "#! gcloud ai models delete $MODEL_NAME --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "#! gsutil -m rm -r $JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand we packaged our XGBoost model and started a custom training job on Vertex AI we can take the ML workflow and formalize it into a Vertex AI Pipeline.\n",
    "\n",
    "You can continue with the next Notebook: `06_formalization.ipynb`."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
