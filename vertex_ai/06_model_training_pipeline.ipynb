{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Fraudfinder - ML Pipeline\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/fraudfinder/raw/main/06_model_training_pipeline.ipynb\">\n",
    "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Google Cloud Notebooks\">Open in Cloud Notebook\n",
    "    </a>\n",
    "  </td> \n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/fraudfinder/blob/main/06_model_training_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/fraudfinder/blob/main/06_model_training_pipeline.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Fraudfinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the Fraudfinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This notebook shows how to use Feature Store, Pipelines and Model Monitoring for building an end-to-end demo using both components defined in `google_cloud_pipeline_components` and custom components. \n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "- [BigQuery](https://cloud.google.com/bigquery/)\n",
    "\n",
    "Steps performed in this notebook:\n",
    "\n",
    "    * Create a Feature Store for store and sharing features\n",
    "    * Create a Pipeline to deploy the model\n",
    "    * Create a Model Monitoring Job to check the status of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "#General\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "#Vertex Pipelines\n",
    "from typing import NamedTuple\n",
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, Metrics, ClassificationMetrics, Condition, component\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google_cloud_pipeline_components import aiplatform as vertex_ai_components\n",
    "from kfp.v2.google.client import AIPlatformClient as VertexAIClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"kfp version:\", kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Components\n",
    "# BASE_IMAGE=\"gcr.io/google.com/cloudsdktool/cloud-sdk:latest\"\n",
    "BASE_IMAGE='python:3.7'\n",
    "COMPONENTS_DIR=os.path.join(os.curdir, 'pipelines', 'components')\n",
    "INGEST_FEATURE_STORE=f\"{COMPONENTS_DIR}/ingest_feature_store_{ID}.yaml\"\n",
    "EVALUATE=f\"{COMPONENTS_DIR}/evaluate_{ID}.yaml\"\n",
    "\n",
    "#Pipeline\n",
    "PIPELINE_NAME = f'fraud-finder-xgb-pipeline-{ID}'\n",
    "PIPELINE_DIR=os.path.join(os.curdir, 'pipelines')\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipelines\"\n",
    "PIPELINE_PACKAGE_PATH = f\"{PIPELINE_DIR}/pipeline_{ID}.json\"\n",
    "\n",
    "#Feature Store component\n",
    "START_DATE_TRAIN = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE_TRAIN = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "BQ_DATASET = \"tx\"\n",
    "READ_INSTANCES_TABLE = f\"ground_truth_{END_DATE_TRAIN.replace('-', '')}\"\n",
    "READ_INSTANCES_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
    "\n",
    "#Dataset component\n",
    "DATASET_NAME = f'fraud_finder_dataset_{END_DATE_TRAIN}'\n",
    "# GCS_SOURCE = [f'{BUCKET_NAME}/data/train/000000000000.csv', f'{BUCKET_NAME}/data/train/000000000001.csv', f'{BUCKET_NAME}/data/train/000000000002.csv']\n",
    "\n",
    "#Training component\n",
    "JOB_NAME = f'fraudfinder-train-xgb-{ID}'\n",
    "MODEL_NAME = f'fraudfinder-xgb-{ID}'\n",
    "TRAIN_MACHINE_TYPE = 'n2-standard-4'\n",
    "CONTAINER_URI = 'us-docker.pkg.dev/vertex-ai/training/xgboost-cpu.1-1:latest'\n",
    "MODEL_SERVING_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-1:latest'\n",
    "PYTHON_MODULE = 'trainer.train_model'\n",
    "ARGS=[\"--bucket\", f\"gs://{BUCKET_NAME}\"]\n",
    "IMAGE_REPOSITORY = f'fraudfinder-{ID}'\n",
    "IMAGE_NAME='dask-xgb-classificator'\n",
    "IMAGE_TAG='v1'\n",
    "IMAGE_URI=f\"us-central1-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\" # TODO: get it from config\n",
    "\n",
    "#Evaluation component\n",
    "METRICS_URI = f\"gs://{BUCKET_NAME}/deliverables/metrics.json\"\n",
    "AVG_PR_THRESHOLD = 0.8\n",
    "AVG_PR_CONDITION = 'avg_pr_condition'\n",
    "\n",
    "#endpoint\n",
    "ENDPOINT_NAME = 'fraudfinder_xgb_prediction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertex AI client\n",
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p -m 777 $PIPELINE_DIR $COMPONENTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define feature store component\n",
    "\n",
    "Notice that the component assumes that containes the entities-timestamps \"query\" is already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil ubla set on gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file=INGEST_FEATURE_STORE, \n",
    "           base_image=BASE_IMAGE, \n",
    "           packages_to_install=[\"git+https://github.com/googleapis/python-aiplatform.git@main\"])\n",
    "\n",
    "def ingest_features_gcs(project_id:str, region:str, bucket_name:str,\n",
    "                       feature_store_id: str, read_instances_uri:str) -> NamedTuple(\"Outputs\",\n",
    "                                                                       [(\"snapshot_uri_paths\", str),],):\n",
    "    \n",
    "    # Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "    from datetime import datetime\n",
    "    import glob\n",
    "    import urllib\n",
    "    import json\n",
    "    \n",
    "    #Feature Store\n",
    "    from google.cloud.aiplatform import Featurestore, EntityType, Feature\n",
    "    \n",
    "    # Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    api_endpoint = region + \"-aiplatform.googleapis.com\"\n",
    "    bucket = urllib.parse.urlsplit(bucket_name).netloc\n",
    "    export_uri = f'{bucket_name}/data/snapshots/{timestamp}' #format as new gsfuse requires\n",
    "    export_uri_path = f'/gcs/{bucket}/data/snapshots/{timestamp}'\n",
    "    customer_entity = 'customer'\n",
    "    terminal_entity = 'terminal'\n",
    "    serving_feature_ids = {customer_entity: ['*'], terminal_entity: ['*']}\n",
    "    \n",
    "    # Main -------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    ## Define the feature store resource path\n",
    "    feature_store_resource_path = f\"projects/{project_id}/locations/{region}/featurestores/{feature_store_id}\"\n",
    "    print(\"Feature Store: \\t\", feature_store_resource_path)\n",
    "    \n",
    "    ## Run batch job request\n",
    "    try:\n",
    "        ff_feature_store = Featurestore(feature_store_resource_path)\n",
    "        ff_feature_store.batch_serve_to_gcs(\n",
    "            gcs_destination_output_uri_prefix = export_uri,\n",
    "            gcs_destination_type = 'csv',\n",
    "            serving_feature_ids = serving_feature_ids,\n",
    "            read_instances_uri = read_instances_uri,\n",
    "            pass_through_fields = ['tx_fraud','tx_amount']\n",
    "        )\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    \n",
    "    #Store metadata\n",
    "    snapshot_pattern = f'{export_uri_path}/*.csv'\n",
    "    snapshot_files = glob.glob(snapshot_pattern)\n",
    "    snapshot_files_fmt = [p.replace('/gcs/', 'gs://') for p in snapshot_files]\n",
    "    snapshot_files_string = json.dumps(snapshot_files_fmt)\n",
    "    \n",
    "    component_outputs = NamedTuple(\"Outputs\",\n",
    "                                [(\"snapshot_uri_paths\", str),],)\n",
    "    \n",
    "    return component_outputs(snapshot_files_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define an evaluate custom component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    output_component_file=EVALUATE\n",
    ")\n",
    "def evaluate_model(\n",
    "    model_in: Input[Artifact],\n",
    "    metrics_uri: str,\n",
    "    meta_metrics: Output[Metrics],\n",
    "    graph_metrics: Output[ClassificationMetrics],\n",
    "    model_out: Output[Artifact]) -> NamedTuple(\"Outputs\",\n",
    "                                            [(\"metrics_thr\", float),],):\n",
    "    \n",
    "    # Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "    import json\n",
    "    \n",
    "    # Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "    metrics_path = metrics_uri.replace('gs://', '/gcs/')\n",
    "    labels = ['not fraud', 'fraud']\n",
    "    \n",
    "    # Main -------------------------------------------------------------------------------------------------------------------------------\n",
    "    with open(metrics_path, mode='r') as json_file:\n",
    "        metrics = json.load(json_file)\n",
    "\n",
    "    ## metrics\n",
    "    fpr = metrics['fpr']\n",
    "    tpr = metrics['tpr']\n",
    "    thrs = metrics['thrs']\n",
    "    c_matrix = metrics['confusion_matrix']\n",
    "    avg_precision_score = metrics['avg_precision_score']\n",
    "    f1 = metrics['f1_score']\n",
    "    lg_loss = metrics['log_loss']\n",
    "    prec_score = metrics['precision_score']\n",
    "    rec_score = metrics['recall_score']\n",
    "    \n",
    "    meta_metrics.log_metric('avg_precision_score', avg_precision_score)\n",
    "    meta_metrics.log_metric('f1_score', f1)\n",
    "    meta_metrics.log_metric('log_loss', lg_loss)\n",
    "    meta_metrics.log_metric('precision_score', prec_score)\n",
    "    meta_metrics.log_metric('recall_score', rec_score)\n",
    "    graph_metrics.log_roc_curve(fpr, tpr, thrs)\n",
    "    graph_metrics.log_confusion_matrix(labels, c_matrix)\n",
    "    \n",
    "    ## model metadata\n",
    "    model_framework = 'xgb.dask'\n",
    "    model_type = 'DaskXGBClassifier'\n",
    "    model_user = 'author' \n",
    "    model_function = 'classification'\n",
    "    model_out.metadata[\"framework\"] = model_framework\n",
    "    model_out.metadata[\"type\"] = model_type\n",
    "    model_out.metadata[\"model function\"] = model_function\n",
    "    model_out.metadata[\"modified by\"] = model_user\n",
    "    \n",
    "    component_outputs = NamedTuple(\"Outputs\",\n",
    "                                [(\"metrics_thr\", float),],)\n",
    "    \n",
    "    return component_outputs(float(avg_precision_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pipeline using ```kfp``` and ```google_cloud_pipeline_components```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=PIPELINE_NAME,\n",
    ")\n",
    "def pipeline(project_id:str = PROJECT_ID, \n",
    "             region:str = REGION, \n",
    "             bucket_name:str = f\"gs://{BUCKET_NAME}\",\n",
    "             feature_store_id:str = FEATURESTORE_ID, \n",
    "             read_instances_uri:str = READ_INSTANCES_URI,\n",
    "             replica_count:int = 1,\n",
    "             machine_type:str = \"n1-standard-4\",\n",
    "             train_split:float = 0.8,\n",
    "             test_split:float = 0.1,\n",
    "             val_split:float = 0.1,\n",
    "             metrics_uri: str = METRICS_URI, \n",
    "             thold: float = AVG_PR_THRESHOLD,\n",
    "            ):\n",
    "    \n",
    "    #Ingest data from featurestore\n",
    "    ingest_features_op = ingest_features_gcs(project_id=project_id, region=region, bucket_name=bucket_name, \n",
    "                                             feature_store_id=feature_store_id, read_instances_uri=read_instances_uri)\n",
    "    \n",
    "    #create dataset \n",
    "    dataset_create_op = vertex_ai_components.TabularDatasetCreateOp(project=project_id,\n",
    "                                                       display_name=DATASET_NAME,\n",
    "                                                       gcs_source=ingest_features_op.outputs['snapshot_uri_paths']).after(ingest_features_op)\n",
    "    \n",
    "    #custom training job component - script\n",
    "    train_model_op = vertex_ai_components.CustomContainerTrainingJobRunOp(\n",
    "        display_name=JOB_NAME,\n",
    "        model_display_name=MODEL_NAME,\n",
    "        container_uri=IMAGE_URI,\n",
    "        staging_bucket=bucket_name,\n",
    "        dataset=dataset_create_op.outputs['dataset'],\n",
    "        base_output_dir=bucket_name,\n",
    "        args = ARGS,\n",
    "        replica_count= replica_count,\n",
    "        machine_type= machine_type,\n",
    "        training_fraction_split=train_split,\n",
    "        validation_fraction_split=val_split,\n",
    "        test_fraction_split=test_split,\n",
    "        model_serving_container_image_uri=MODEL_SERVING_IMAGE_URI,\n",
    "        project=project_id,\n",
    "        location=region).after(dataset_create_op)\n",
    "    \n",
    "    #evaluate component\n",
    "    evaluate_model_op = evaluate_model(model_in=train_model_op.outputs[\"model\"], \n",
    "                                       metrics_uri=metrics_uri).after(train_model_op)\n",
    "    \n",
    "    #if threshold\n",
    "    with Condition(evaluate_model_op.outputs['metrics_thr'] > thold, name=AVG_PR_CONDITION):\n",
    "    \n",
    "        #create endpoint\n",
    "        create_endpoint_op = vertex_ai_components.EndpointCreateOp(\n",
    "            display_name=ENDPOINT_NAME,\n",
    "            project=project_id).after(evaluate_model_op)\n",
    "\n",
    "        #deploy th model\n",
    "        custom_model_deploy_op = vertex_ai_components.ModelDeployOp(\n",
    "            model=train_model_op.outputs[\"model\"],\n",
    "            endpoint=create_endpoint_op.outputs[\"endpoint\"],\n",
    "            deployed_model_display_name=MODEL_NAME,\n",
    "            dedicated_resources_machine_type=machine_type,\n",
    "            dedicated_resources_min_replica_count=replica_count\n",
    "        ).after(create_endpoint_op)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_compiler = compiler.Compiler()\n",
    "pipeline_compiler.compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=PIPELINE_PACKAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate pipeline representation\n",
    "pipeline_job = vertex_ai.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINE_PACKAGE_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job.run(sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "{TODO: Include commands to delete individual resources below}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Delete endpoint resource\n",
    "! gcloud ai endpoints delete $ENDPOINT_NAME --quiet --region $REGION_NAME\n",
    "\n",
    "# Delete model resource\n",
    "! gcloud ai models delete $MODEL_NAME --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "! gsutil -m rm -r $JOB_DIR"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
