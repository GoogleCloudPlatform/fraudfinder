{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Fraudfinder - Feature engineering (batch)\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name=Model%20Monitoring&download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmaster%2Fnotebooks%2Fcommunity%2Fmodel_monitoring%2Fmodel_monitoring_feature_attribs.ipynb\">\n",
    "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Google Cloud Notebooks\">Open in Cloud Notebook\n",
    "    </a>\n",
    "  </td> \n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/model_monitoring/model_monitoring_feature_attribs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/model_monitoring/model_monitoring_feature_attribs.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Fraudfinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the Fraudfinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This notebook shows how to generate new features on bank transactions by customer and terminal, by doing batch feature engineering in SQL with BigQuery. Then, you will create a feature store using Vertex AI Feature Store, and ingest your newly-created features from BigQuery into Vertex AI Feature Store, so that a feature store can become the single source of data for both training and model inference. \n",
    "\n",
    "In the following notebook, you will learn:\n",
    "\n",
    "- How to use BigQuery for feature engineering\n",
    "- Create a feature store \n",
    "- Ingest features into the feature store\n",
    "- Read features from the feature store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "First, set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1644823642503,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Union\n",
    "import json\n",
    "\n",
    "# Data Engineering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Vertex AI and Vertex AI Feature Store \n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import Featurestore, EntityType, Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Moq4QZKjY4fv"
   },
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1160,
     "status": "ok",
     "timestamp": 1644828029574,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "G-4MU7kF3t4x"
   },
   "outputs": [],
   "source": [
    "# Organize project directory structure.\n",
    "# DATA_DIR = os.path.join(\"..\", \"data\")\n",
    "# RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw\")\n",
    "\n",
    "# Define the range of transactions for training (Jan 2022).\n",
    "YEAR_MONTH_PREFIX = \"2022-01\"\n",
    "DATAPROCESSING_START_DATE = f\"{YEAR_MONTH_PREFIX}-01\"\n",
    "DATAPROCESSING_END_DATE = f\"{YEAR_MONTH_PREFIX}-31\"\n",
    "\n",
    "# Define BiqQuery dataset, tables, and time windows to calculate static behavioral features.\n",
    "RAW_TABLE_LABELS = \"txlabels\"\n",
    "FEATURES_TABLE_NAME = f\"{PROJECT_ID}.tx.wide_features_table\"\n",
    "FEATURES_PARTIONED_TABLE = f\"{FEATURES_TABLE_NAME}_{YEAR_MONTH_PREFIX.replace('-', '')}*\"\n",
    "\n",
    "# Define Vertex AI Feature store settings.\n",
    "ONLINE_STORAGE_NODES = 1\n",
    "FEATURE_TIME = \"feature_ts\"\n",
    "CUSTOMER_ENTITY_ID = \"customer\"\n",
    "TERMINAL_ENTITY_ID = \"terminal\"\n",
    "CUSTOMERS_TABLE_NAME = f\"{PROJECT_ID}.tx.customers_{DATAPROCESSING_END_DATE.replace('-', '')}\"\n",
    "TERMINALS_TABLE_NAME = f\"{PROJECT_ID}.tx.terminals_{DATAPROCESSING_END_DATE.replace('-', '')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bq_query(sql: str, project: str, region: str, return_df=False) -> Union[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    An helper function to run a BigQuery query\n",
    "    Args:\n",
    "        sql: BigQuery query\n",
    "        project: project id\n",
    "        region: region\n",
    "        debug: dry run the query\n",
    "        return_df: return a dataframe or not\n",
    "    Returns:\n",
    "        df: BigQuery query result\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a BigQuery client.\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Try dry run before executing query to catch any errors\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    # Proceed to run query\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client_result = bq_client.query(sql, job_config=job_config)\n",
    "    result = client_result.result()\n",
    "    job_id = client_result.job_id\n",
    "    print(f\"Finished job_id: {job_id}\")\n",
    "\n",
    "    if return_df:\n",
    "        # Get & return data frame\n",
    "        df = result.to_arrow().to_pandas()\n",
    "        return df\n",
    "    \n",
    "    \n",
    "def create_batch_features(features_table_name: str, \n",
    "                          query: str, \n",
    "                          end_date: str, \n",
    "                          backfill_interval: int, \n",
    "                          project_id: str, \n",
    "                          region: str\n",
    "                         ) -> List[str]:\n",
    "    \"\"\"\n",
    "    A helper function to create batch features\n",
    "    Args:\n",
    "        features_table_name: name of the feature table\n",
    "        query: query to create the feature table\n",
    "        backfill_interval: backfill interval\n",
    "        query_param: query parameter to change\n",
    "        project_id: project id\n",
    "        region: region\n",
    "    Returns:\n",
    "        features_table_names: list of feature tables\n",
    "    \"\"\"\n",
    "    # Initialize the backfill starting date.\n",
    "    backfill_start_date = datetime.strptime(end_date, \"%Y-%m-%d\") - timedelta(days=backfill_interval)\n",
    "\n",
    "    # Initialize a list to contain feature table names.\n",
    "    features_table_names = []\n",
    "\n",
    "    try:\n",
    "        # Create a BigQuery client.\n",
    "        bq_client = bigquery.Client(project=project_id, location=region)\n",
    "\n",
    "        # For each day in the backfill interval\n",
    "        for dix in range(backfill_interval + 1):\n",
    "            \n",
    "            # Get the backfill date.\n",
    "            date_query = backfill_start_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            # Create the feature table name.\n",
    "            destination = f'{features_table_name}_{backfill_start_date.strftime(\"%Y%m%d\")}'\n",
    "\n",
    "            # Create the query.\n",
    "            query = query.replace(\"@END_DATE_TRAIN\", f\"'{date_query}'\")\n",
    "\n",
    "            # Create the query job.\n",
    "            job_config = bigquery.QueryJobConfig(destination=destination, write_disposition='WRITE_TRUNCATE')\n",
    "\n",
    "            # Run the query.\n",
    "            job = bq_client.query(query, job_config=job_config)\n",
    "            _ = job.result()\n",
    "\n",
    "            # Append the feature table name to the list.\n",
    "            features_table_names.append(destination)\n",
    "\n",
    "            # Increment the backfill date.\n",
    "            backfill_start_date += timedelta(days=1)\n",
    "\n",
    "    except RuntimeError as error:\n",
    "        print(error)\n",
    "\n",
    "    return features_table_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Initialize Vertex AI SDK\n",
    "Initialize the Vertex AI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKaROgJZ3t4y"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NDaDFyj3t4z"
   },
   "source": [
    "### Customer ID and Terminal ID transformations\n",
    "\n",
    "In this section, you will create features, based on historical customer behaviour and historical terminal activity, which can be later used to train a machine learning model. \n",
    "\n",
    "Shown below are some SQL queries used to generate the wide features table you'll want to ingest into feature store. \n",
    "\n",
    "The query will calculate 2 sets of features: \n",
    "\n",
    "1. Customer features which describes the spending behaviour of customer within 7, 14 and 30 days time windows using number of transactions and avarage amount spent\n",
    "\n",
    "2. Terminal features which describes the risk of a given terminal to be exposed to fraudulent transactions within 7, 14 and 30 days using average number of fraudulent transactions and the number of transactions. One thing to notice is that we add some delay which would take into account time would pass between the time of transaction and the result of fraud investigation or customer claim.\n",
    "\n",
    "You will use one month of transaction data starting from the end of January and going back. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1644823783649,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "43Ck_vAc3t4z"
   },
   "outputs": [],
   "source": [
    "create_batch_features_query = \\\n",
    "f\"\"\"\n",
    "WITH\n",
    "  # query to join labels with features -------------------------------------------------------------------------------------------\n",
    "  get_raw_table AS (\n",
    "  SELECT\n",
    "    raw_tx.TX_TS,\n",
    "    raw_tx.TX_ID,\n",
    "    raw_tx.CUSTOMER_ID,\n",
    "    raw_tx.TERMINAL_ID,\n",
    "    raw_tx.TX_AMOUNT,\n",
    "    raw_lb.TX_FRAUD\n",
    "  FROM (\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      `tx.tx`\n",
    "    WHERE\n",
    "      DATE(TX_TS) <= @END_DATE_TRAIN\n",
    "    ORDER BY\n",
    "      TX_TS) AS raw_tx\n",
    "  LEFT JOIN \n",
    "    `tx.{RAW_TABLE_LABELS}` as raw_lb\n",
    "  ON raw_tx.TX_ID = raw_lb.TX_ID),\n",
    "\n",
    "  # query to calculate CUSTOMER spending behaviour --------------------------------------------------------------------------------\n",
    "  get_customer_spending_behaviour AS (\n",
    "  SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    TX_AMOUNT,\n",
    "    TX_FRAUD,\n",
    "    \n",
    "    # calculate the number of customer transactions over windows\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 86400 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_NB_TX_1DAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_NB_TX_7DAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_NB_TX_14DAY_WINDOW,\n",
    "      \n",
    "    # calculate the customer average transactions amount over windows\n",
    "    AVG(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 86400 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW,\n",
    "    AVG(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW,\n",
    "    AVG(TX_AMOUNT) OVER (PARTITION BY CUSTOMER_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS CUSTOMER_ID_AVG_AMOUNT_14DAY_WINDOW,\n",
    "  FROM get_raw_table),\n",
    "\n",
    "  # query to calculate TERMINAL spending behaviour --------------------------------------------------------------------------------\n",
    "  get_variables_delay_window AS (\n",
    "  SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    \n",
    "    # calculate total amount and the total number of trasactions over the delay period (7 days - delay)\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_DELAY,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 604800 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_DELAY,\n",
    "      \n",
    "    # calculate total amount and the total number of trasactions over the delayed window (window + 7 days - delay)\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 691200 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_1_DELAY_WINDOW,\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_7_DELAY_WINDOW,\n",
    "    SUM(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1814400 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_FRAUD_14_DELAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 691200 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_1_DELAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1209600 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_7_DELAY_WINDOW,\n",
    "    COUNT(TX_FRAUD) OVER (PARTITION BY TERMINAL_ID ORDER BY UNIX_SECONDS(TX_TS) ASC RANGE BETWEEN 1814400 PRECEDING\n",
    "      AND CURRENT ROW ) AS NB_TX_14_DELAY_WINDOW,\n",
    "  FROM get_raw_table),\n",
    "\n",
    "  # query to calculate TERMINAL risk factors ---------------------------------------------------------------------------------------\n",
    "  get_risk_factors AS (\n",
    "  SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    # calculate numerator\n",
    "    NB_FRAUD_1_DELAY_WINDOW - NB_FRAUD_DELAY AS TERMINAL_ID_NB_FRAUD_1DAY_WINDOW,\n",
    "    NB_FRAUD_7_DELAY_WINDOW - NB_FRAUD_DELAY AS TERMINAL_ID_NB_FRAUD_7DAY_WINDOW,\n",
    "    NB_FRAUD_14_DELAY_WINDOW - NB_FRAUD_DELAY AS TERMINAL_ID_NB_FRAUD_14DAY_WINDOW,\n",
    "    # calculate denominator\n",
    "    NB_TX_1_DELAY_WINDOW - NB_TX_DELAY AS TERMINAL_ID_NB_TX_1DAY_WINDOW,\n",
    "    NB_TX_7_DELAY_WINDOW - NB_TX_DELAY AS TERMINAL_ID_NB_TX_7DAY_WINDOW,\n",
    "    NB_TX_14_DELAY_WINDOW - NB_TX_DELAY AS TERMINAL_ID_NB_TX_14DAY_WINDOW,\n",
    "      FROM\n",
    "    get_variables_delay_window),\n",
    "\n",
    "  # query to calculate the TERMINAL risk index -------------------------------------------------------------------------------------\n",
    "  get_risk_index AS (\n",
    "    SELECT\n",
    "    TX_TS,\n",
    "    TX_ID,\n",
    "    CUSTOMER_ID,\n",
    "    TERMINAL_ID,\n",
    "    TERMINAL_ID_NB_TX_1DAY_WINDOW,\n",
    "    TERMINAL_ID_NB_TX_7DAY_WINDOW,\n",
    "    TERMINAL_ID_NB_TX_14DAY_WINDOW,\n",
    "    # calculate the risk index\n",
    "    (TERMINAL_ID_NB_FRAUD_1DAY_WINDOW/(TERMINAL_ID_NB_TX_1DAY_WINDOW+0.0001)) AS TERMINAL_ID_RISK_1DAY_WINDOW,\n",
    "    (TERMINAL_ID_NB_FRAUD_7DAY_WINDOW/(TERMINAL_ID_NB_TX_7DAY_WINDOW+0.0001)) AS TERMINAL_ID_RISK_7DAY_WINDOW,\n",
    "    (TERMINAL_ID_NB_FRAUD_14DAY_WINDOW/(TERMINAL_ID_NB_TX_14DAY_WINDOW+0.0001)) AS TERMINAL_ID_RISK_14DAY_WINDOW\n",
    "    FROM get_risk_factors \n",
    "  )\n",
    "\n",
    "SELECT\n",
    "  PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', CONCAT(@END_DATE_TRAIN, ' ', STRING(TIME_TRUNC(CURRENT_TIME(), SECOND))), 'UTC') AS feature_ts,\n",
    "  a.CUSTOMER_ID AS customer_id,\n",
    "  a.TERMINAL_ID AS terminal_id,\n",
    "  CAST(a.CUSTOMER_ID_NB_TX_1DAY_WINDOW AS INT64) AS customer_id_nb_tx_1day_window,\n",
    "  CAST(a.CUSTOMER_ID_NB_TX_7DAY_WINDOW AS INT64) AS customer_id_nb_tx_7day_window,\n",
    "  CAST(a.CUSTOMER_ID_NB_TX_14DAY_WINDOW AS INT64) AS customer_id_nb_tx_14day_window,\n",
    "  CAST(a.CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW AS FLOAT64) AS customer_id_avg_amount_1day_window,\n",
    "  CAST(a.CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW AS FLOAT64) AS customer_id_avg_amount_7day_window,\n",
    "  CAST(a.CUSTOMER_ID_AVG_AMOUNT_14DAY_WINDOW AS FLOAT64) AS customer_id_avg_amount_14day_window,\n",
    "  CAST(b.TERMINAL_ID_NB_TX_1DAY_WINDOW AS INT64) AS terminal_id_nb_tx_1day_window,\n",
    "  CAST(b.TERMINAL_ID_NB_TX_7DAY_WINDOW AS INT64) AS terminal_id_nb_tx_7day_window,\n",
    "  CAST(b.TERMINAL_ID_NB_TX_14DAY_WINDOW AS INT64) AS terminal_id_nb_tx_14day_window,\n",
    "  CAST(b.TERMINAL_ID_RISK_1DAY_WINDOW AS FLOAT64) AS terminal_id_risk_1day_window,\n",
    "  CAST(b.TERMINAL_ID_RISK_7DAY_WINDOW AS FLOAT64) AS terminal_id_risk_7day_window,\n",
    "  CAST(b.TERMINAL_ID_RISK_14DAY_WINDOW AS FLOAT64) AS terminal_id_risk_14day_window,\n",
    "FROM\n",
    "  get_customer_spending_behaviour AS a\n",
    "INNER JOIN\n",
    "  get_risk_index AS b\n",
    "ON\n",
    "  a.TX_TS = b.TX_TS\n",
    "  AND a.TX_ID = b.TX_ID\n",
    "  AND a.CUSTOMER_ID = b.CUSTOMER_ID\n",
    "  AND a.TERMINAL_ID = b.TERMINAL_ID\n",
    "ORDER BY\n",
    "  a.TX_TS\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the query above to aggregate and store the wide feature data in a BigQuery table.\n",
    "\n",
    "In the code below, you will iterate the SQL over predifined number of days in order to  backfill features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKFILL_INTERVAL = 1\n",
    "BACKFILL_START_DATE = datetime.strptime(DATAPROCESSING_END_DATE, \"%Y-%m-%d\") - timedelta(days=BACKFILL_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_table_names = create_batch_features(\n",
    "    query=create_batch_features_query,\n",
    "    features_table_name=FEATURES_TABLE_NAME,\n",
    "    end_date=DATAPROCESSING_END_DATE, \n",
    "    backfill_interval=BACKFILL_INTERVAL, \n",
    "    project_id=PROJECT_ID, \n",
    "    region=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick peek at the BigQuery data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_query = f\"\"\"SELECT * FROM `{features_table_names[-1]}` LIMIT 10\"\"\"\n",
    "batch_feature_table_df = run_bq_query(view_query, project=PROJECT_ID, region=REGION, return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_feature_table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize real time features \n",
    "\n",
    "In order to ingest realtime features, we initialize them with a default values. The following query will\n",
    "\n",
    "- Add one column for each real time feature\n",
    "- Set 0 as default values for each of them\n",
    "- Update all real-time columns with default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiate_real_time_features_query = \\\n",
    "f\"\"\"\n",
    "ALTER TABLE `@FEATURE_TABLE`\n",
    "ADD COLUMN customer_id_nb_tx_15min_window INT64,\n",
    "ADD COLUMN customer_id_nb_tx_30min_window INT64,\n",
    "ADD COLUMN customer_id_nb_tx_60min_window INT64,\n",
    "ADD COLUMN customer_id_avg_amount_15min_window FLOAT64,\n",
    "ADD COLUMN customer_id_avg_amount_30min_window FLOAT64,\n",
    "ADD COLUMN customer_id_avg_amount_60min_window FLOAT64,\n",
    "ADD COLUMN terminal_id_nb_tx_15min_window INT64,\n",
    "ADD COLUMN terminal_id_nb_tx_30min_window INT64,\n",
    "ADD COLUMN terminal_id_nb_tx_60min_window INT64,\n",
    "ADD COLUMN terminal_id_avg_amount_15min_window FLOAT64,\n",
    "ADD COLUMN terminal_id_avg_amount_30min_window FLOAT64,\n",
    "ADD COLUMN terminal_id_avg_amount_60min_window FLOAT64;\n",
    "\n",
    "ALTER TABLE `@FEATURE_TABLE`\n",
    "ALTER COLUMN customer_id_nb_tx_15min_window SET DEFAULT 0,\n",
    "ALTER COLUMN customer_id_nb_tx_30min_window SET DEFAULT 0,\n",
    "ALTER COLUMN customer_id_nb_tx_60min_window SET DEFAULT 0,\n",
    "ALTER COLUMN customer_id_avg_amount_15min_window SET DEFAULT 0,\n",
    "ALTER COLUMN customer_id_avg_amount_30min_window SET DEFAULT 0,\n",
    "ALTER COLUMN customer_id_avg_amount_60min_window SET DEFAULT 0,\n",
    "ALTER COLUMN terminal_id_nb_tx_15min_window SET DEFAULT 0,\n",
    "ALTER COLUMN terminal_id_nb_tx_30min_window SET DEFAULT 0,\n",
    "ALTER COLUMN terminal_id_nb_tx_60min_window SET DEFAULT 0,\n",
    "ALTER COLUMN terminal_id_avg_amount_15min_window SET DEFAULT 0,\n",
    "ALTER COLUMN terminal_id_avg_amount_30min_window SET DEFAULT 0,\n",
    "ALTER COLUMN terminal_id_avg_amount_60min_window SET DEFAULT 0;\n",
    "\n",
    "UPDATE `@FEATURE_TABLE`\n",
    "SET customer_id_nb_tx_15min_window = 0,\n",
    "    customer_id_nb_tx_30min_window  = 0,\n",
    "    customer_id_nb_tx_60min_window  = 0, \n",
    "    customer_id_avg_amount_15min_window = 0,\n",
    "    customer_id_avg_amount_30min_window  = 0,\n",
    "    customer_id_avg_amount_60min_window  = 0,\n",
    "    terminal_id_nb_tx_15min_window = 0,\n",
    "    terminal_id_nb_tx_30min_window  = 0,\n",
    "    terminal_id_nb_tx_60min_window  = 0,\n",
    "    terminal_id_avg_amount_15min_window = 0,\n",
    "    terminal_id_avg_amount_30min_window = 0,\n",
    "    terminal_id_avg_amount_60min_window  = 0\n",
    "WHERE TRUE; \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the query above to initialize the real-time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tbn in features_table_names:\n",
    "    initialization_query = initiate_real_time_features_query\n",
    "    initialization_query = initialization_query.replace('@FEATURE_TABLE', tbn)\n",
    "    run_bq_query(initialization_query, project=PROJECT_ID, region=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick peek at the BigQuery data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_query = f\"\"\"SELECT * FROM `{features_table_names[-1]}` LIMIT 10\"\"\"\n",
    "feature_table_df = run_bq_query(view_query, project=PROJECT_ID, region=REGION, return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9xB8hVU3t40"
   },
   "source": [
    "## Feature Store for feature management\n",
    "\n",
    "### What is a feature store?\n",
    "\n",
    "The features generated are great examples of features that we can store the [Vertex AI Feature Store](https://cloud.google.com/vertex-ai/docs/featurestore). This is because:\n",
    "\n",
    "- The features need to be precalculated.\n",
    "- We want to share the features. \n",
    "- We will use the features for real-time predictions. \n",
    "\n",
    "Vertex AI Feature Store provides a centralized repository for organizing, storing, and serving ML features. Using a central featurestore enables an organization to efficiently share, discover, and re-use ML features at scale, which can increase the velocity of developing and deploying new ML applications.\n",
    "\n",
    "### Why would you like to set up it?\n",
    "\n",
    "So far you've built and stored features in BigQuery. Now, in order to predict fraud, you want to serve those features in real-time with millisecond scale latency. In particular, when the ML gateway receives a prediction request for a specific transaction (including customer, terminal, and transaction ids), the system needs to fetch the features related to that transaction and pass them as inputs to the model for online prediction. As you can imagine, an analytical data warehouse such as BigQuery is not able to provide low-latency near real-time read operations. \n",
    "\n",
    "Last year, Google Cloud announched Vertex AI, a managed machine learning (ML) platform that allows data science teams to accelerate the deployment and maintenance of ML models. The platform is comprised of several building blocks, including the Vertex AI Feature Store, which provides a managed service for low latency scalable feature serving. It also provides a centralized feature repository with easy APIs to search and discover features, as well as feature monitoring capabilities to track drift and other quality issues. \n",
    "\n",
    "Vertex AI Feature Store uses a time series data model to store a series of values for features. This enables Vertex AI Feature Store to maintain feature values as they change over time and to support point-in-time queries of feature values. Feature Store organizes resources hierarchically (`Featurestore -> EntityType -> Feature`) in the following order: \n",
    "\n",
    "- **Featurestore**: the resource to contains entities and features.\n",
    "    - **EntityType**: under a Featurestore, an EntityType describes an minimal data entry.\n",
    "        - **Feature**: under an EntityType, a feature is an attribute of the EntityType. \n",
    "\n",
    "\n",
    "You must create these resources before you can ingest data into a Feature Store. \n",
    "\n",
    "Let's do that now using the **Vertex AI SDK**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfKeQmf63t42"
   },
   "source": [
    "### Create featurestore, `fraud_detection`\n",
    "\n",
    "A featurestore is the top-level container for entity types, features, and feature values. Typically, an organization creates one shared featurestore for feature ingestion, serving, and sharing across all teams in the organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10606,
     "status": "ok",
     "timestamp": 1644830323473,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "nQmwn7agJZ6o",
    "outputId": "4d7af5e1-93c4-4286-adc8-46445ac9a00e"
   },
   "outputs": [],
   "source": [
    "# Try to create a new featurestore resource\n",
    "ff_feature_store = Featurestore.create(featurestore_id=f'{FEATURESTORE_ID}',\n",
    "                              online_store_fixed_node_count=ONLINE_STORAGE_NODES, \n",
    "                              labels={\"team\": \"dataoffice\", \n",
    "                                      \"app\" : \"fraud_finder\"}, \n",
    "                              sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkhVJqph3t42"
   },
   "source": [
    "### Create the main entity types and their features\n",
    "\n",
    "An entity type is a collection of semantically related features. You define your own entity types, based on the concepts that are relevant to your use case. In this case, the Fraud Finder service has the entity types event, customer and transaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0gimd2Y3t44"
   },
   "source": [
    "#### Create the ```customer``` entity type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1671,
     "status": "ok",
     "timestamp": 1644830643664,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "TBPk3KdRQXec",
    "outputId": "09b4fac9-0b91-4e50-cf79-9dea35f50429"
   },
   "outputs": [],
   "source": [
    "customer_entity_type = ff_feature_store.create_entity_type(\n",
    "                                        entity_type_id=CUSTOMER_ENTITY_ID,\n",
    "                                        description=\"Customer Entity\", \n",
    "                                        sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHglWPU43t45"
   },
   "source": [
    "#### Create features of the ```customer``` entity type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1644830653532,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "Y_TlUTtkRKaN"
   },
   "outputs": [],
   "source": [
    "customer_feature_configs = {\n",
    "    \"customer_id_nb_tx_1day_window\": {\n",
    "    \"value_type\": \"INT64\",\n",
    "    \"description\" : \"Number of transactions by the customer in the last day\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"customer_id_nb_tx_7day_window\": {\n",
    "    \"value_type\": \"INT64\",\n",
    "    \"description\" : \"Number of transactions by the customer in the last 7 days\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"customer_id_nb_tx_14day_window\": {\n",
    "    \"value_type\": \"INT64\",\n",
    "    \"description\" : \"Number of transactions by the customer in the last 14 days\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"customer_id_avg_amount_1day_window\": {\n",
    "    \"value_type\": \"DOUBLE\",\n",
    "    \"description\" : \"Average spending amount in the last day\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"customer_id_avg_amount_7day_window\": {\n",
    "    \"value_type\": \"DOUBLE\",\n",
    "    \"description\" : \"Average spending amount in the last 7 days\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"customer_id_avg_amount_14day_window\": {\n",
    "    \"value_type\": \"DOUBLE\",\n",
    "    \"description\" : \"Average spending amount in the last 14 days\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"customer_id_nb_tx_15min_window\": {\n",
    "    \"value_type\": \"INT64\",\n",
    "    \"description\" : \"Number of transactions by the customer in the last 15 minutes\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"customer_id_nb_tx_30min_window\": {\n",
    "    \"value_type\": \"INT64\",\n",
    "    \"description\" : \"Number of transactions by the customer in the last 30 minutes\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"customer_id_nb_tx_60min_window\": {\n",
    "    \"value_type\": \"INT64\",\n",
    "    \"description\" : \"Number of transactions by the customer in the last 60 minutes\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"customer_id_avg_amount_15min_window\": {\n",
    "    \"value_type\": \"DOUBLE\",\n",
    "    \"description\" : \"Average spending amount in the last 15 minutes\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"customer_id_avg_amount_30min_window\": {\n",
    "    \"value_type\": \"DOUBLE\",\n",
    "    \"description\" : \"Average spending amount in the last 30 minutes\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"customer_id_avg_amount_60min_window\": {\n",
    "    \"value_type\": \"DOUBLE\",\n",
    "    \"description\" : \"Average spending amount in the last 60 minutes\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3200,
     "status": "ok",
     "timestamp": 1644830657996,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "Upho8Vg13t46",
    "outputId": "607c91ec-f21d-4f77-a198-9324c50d483a"
   },
   "outputs": [],
   "source": [
    "customer_feature_ids = customer_entity_type.batch_create_features(\n",
    "        feature_configs = customer_feature_configs,\n",
    "        sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7Qb-5aI3t46"
   },
   "source": [
    "#### Create the ```terminal``` entity type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1597,
     "status": "ok",
     "timestamp": 1644830659588,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "Pje6CLgj3t46",
    "outputId": "668cb2c5-3f59-4645-ba73-6e54ab660cf7"
   },
   "outputs": [],
   "source": [
    "terminal_entity_type = ff_feature_store.create_entity_type(\n",
    "                        entity_type_id=TERMINAL_ENTITY_ID,\n",
    "                        description=\"Terminal Entity\", \n",
    "                        sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tUvpIfR3t47"
   },
   "source": [
    "#### Create features of the ```terminal``` entity type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1644830677787,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "Hwkz6FSeXAUN"
   },
   "outputs": [],
   "source": [
    "terminal_feature_configs = {\n",
    "    \"terminal_id_nb_tx_1day_window\": {\n",
    "    \"value_type\": \"INT64\",\n",
    "    \"description\" : \"Number of transactions by the terminal in the last day\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"terminal_id_nb_tx_7day_window\": {\n",
    "    \"value_type\": \"INT64\",\n",
    "    \"description\" : \"Number of transactions by the terminal in the 7 days\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"terminal_id_nb_tx_14day_window\": {\n",
    "    \"value_type\": \"INT64\",\n",
    "    \"description\" : \"Number of transactions by the terminal in the 14 days\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"terminal_id_risk_1day_window\": {\n",
    "    \"value_type\": \"DOUBLE\",\n",
    "    \"description\" : \"Risk score calculated average number of frauds on the terminal in the last day\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"terminal_id_risk_7day_window\": {\n",
    "    \"value_type\": \"DOUBLE\",\n",
    "    \"description\" : \"Risk score calculated average number of frauds on the terminal in the last 7 days\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"terminal_id_risk_14day_window\": {\n",
    "    \"value_type\": \"DOUBLE\",\n",
    "    \"description\" : \"Risk score calculated average number of frauds on the terminal in the last 14 day\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"terminal_id_nb_tx_15min_window\": {\n",
    "    \"value_type\": \"INT64\",\n",
    "    \"description\" : \"Number of transactions by the terminal in the last 15 minutes\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"terminal_id_nb_tx_30min_window\": {\n",
    "    \"value_type\": \"INT64\",\n",
    "    \"description\" : \"Number of transactions by the terminal in the last 30 minutes\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"terminal_id_nb_tx_60min_window\": {\n",
    "    \"value_type\": \"INT64\",\n",
    "    \"description\" : \"Number of transactions by the terminal in the last 60 minutes\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"terminal_id_avg_amount_15min_window\": {\n",
    "    \"value_type\": \"DOUBLE\",\n",
    "    \"description\" : \"Average spending amount in the last 15 minutes\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"terminal_id_avg_amount_30min_window\": {\n",
    "    \"value_type\": \"DOUBLE\",\n",
    "    \"description\" : \"Average spending amount in the last 30 minutes\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    },\n",
    "    \"terminal_id_avg_amount_60min_window\": {\n",
    "    \"value_type\": \"DOUBLE\",\n",
    "    \"description\" : \"Average spending amount in the last 60 minutes\",\n",
    "    \"labels\": {\"status\": \"passed\"}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3033,
     "status": "ok",
     "timestamp": 1644830682209,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "SxNJmlCbX5HK",
    "outputId": "8e420211-0757-4672-b7d0-75e5f05a6597"
   },
   "outputs": [],
   "source": [
    "terminal_feature_ids = terminal_entity_type.batch_create_features(\n",
    "    feature_configs = terminal_feature_configs,\n",
    "    sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick look in the console\n",
    "\n",
    "Let see how Vertex AI Feature store looks like in the [console](https://console.cloud.google.com/vertex-ai/features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwVoJ42m3t48"
   },
   "source": [
    "### Import feature values \n",
    "\n",
    "Now we need to import the actual feature values before we can use them for online/offline use.\n",
    "\n",
    "About **Source Data format and Layout**:\n",
    "\n",
    "- The feature store [supports source data](https://cloud.google.com/vertex-ai/docs/featurestore/source-data) from BigQuery tables or Avro and CSV files on Google Cloud Storage.\n",
    "- Each imported entity *must* have an ID.\n",
    "- Each entity can *optionally* have a timestamp, to specifying when the feature values are generated.\n",
    "\n",
    "We will now decompose the wide features table (constructed earlier in this notebook) into three sub-tables: events, customers, and terminals. Then we will import feature values from those tables into our feature store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1cSYyAy3t48"
   },
   "source": [
    "#### Prepare tables to import\n",
    "\n",
    "Now it's time to import the data into our Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1644832271645,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "-IpvPd_d3t48"
   },
   "outputs": [],
   "source": [
    "# Define queries to select relevant colums for each entities\n",
    "\n",
    "sql_queries_feature_store = []\n",
    "\n",
    "\n",
    "customers_sql_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{CUSTOMERS_TABLE_NAME}` AS\n",
    "SELECT feature_ts, customer_id, customer_id_nb_tx_1day_window,\n",
    "customer_id_nb_tx_7day_window, customer_id_nb_tx_14day_window,\n",
    "customer_id_avg_amount_1day_window, customer_id_avg_amount_7day_window,\n",
    "customer_id_avg_amount_14day_window, customer_id_nb_tx_15min_window,\n",
    "customer_id_nb_tx_30min_window, customer_id_nb_tx_60min_window,\n",
    "customer_id_avg_amount_15min_window, customer_id_avg_amount_30min_window,\n",
    "customer_id_avg_amount_60min_window\n",
    "FROM `{FEATURES_PARTIONED_TABLE}`\n",
    "ORDER BY feature_ts\n",
    "\"\"\"\n",
    "\n",
    "sql_queries_feature_store.append(customers_sql_query)\n",
    "\n",
    "\n",
    "terminals_sql_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{TERMINALS_TABLE_NAME}` AS\n",
    "SELECT feature_ts, terminal_id, terminal_id_nb_tx_1day_window,\n",
    "terminal_id_nb_tx_7day_window, terminal_id_nb_tx_14day_window,\n",
    "terminal_id_risk_1day_window,terminal_id_risk_7day_window,\n",
    "terminal_id_risk_14day_window, terminal_id_nb_tx_15min_window,\n",
    "terminal_id_nb_tx_30min_window, terminal_id_nb_tx_60min_window,\n",
    "terminal_id_avg_amount_15min_window, terminal_id_avg_amount_30min_window,\n",
    "terminal_id_avg_amount_60min_window\n",
    "FROM `{FEATURES_PARTIONED_TABLE}`\n",
    "ORDER BY feature_ts\n",
    "\"\"\"\n",
    "\n",
    "sql_queries_feature_store.append(terminals_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 135716,
     "status": "ok",
     "timestamp": 1644832407358,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "_-4NWiv53t48"
   },
   "outputs": [],
   "source": [
    "for sql_query in sql_queries_feature_store:\n",
    "    run_bq_query(sql_query, project=PROJECT_ID, region=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick peek at the BigQuery customer and terminal entity tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_query = f\"\"\"SELECT * FROM `{CUSTOMERS_TABLE_NAME}` LIMIT 10\"\"\"\n",
    "customer_table_df = run_bq_query(view_query, project=PROJECT_ID, region=REGION, return_df=True)\n",
    "customer_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_query = f\"\"\"SELECT * FROM `{TERMINALS_TABLE_NAME}` LIMIT 10\"\"\"\n",
    "terminal_table_df = run_bq_query(view_query, project=PROJECT_ID, region=REGION, return_df=True)\n",
    "terminal_table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usby4wvu3t49"
   },
   "source": [
    "#### Import customers\n",
    "\n",
    "In the following section, you will import customers features into your feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1644832428811,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "TVrPDdZi3t4-"
   },
   "outputs": [],
   "source": [
    "CUSTOMERS_FEATURES_IDS = [feature.name for feature in customer_feature_ids.list_features()]\n",
    "CUSTOMER_BQ_SOURCE_URI = f\"bq://{CUSTOMERS_TABLE_NAME}\"\n",
    "CUSTOMER_ENTITY_ID_FIELD = \"customer_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1045268,
     "status": "ok",
     "timestamp": 1644833475223,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "CQ1oDfu5oHXO",
    "outputId": "85a15710-05ce-4d70-a205-6ef469311cf4"
   },
   "outputs": [],
   "source": [
    "customer_entity_type.ingest_from_bq(\n",
    "    feature_ids=CUSTOMERS_FEATURES_IDS,\n",
    "    feature_time=FEATURE_TIME,\n",
    "    bq_source_uri=CUSTOMER_BQ_SOURCE_URI,\n",
    "    entity_id_field=CUSTOMER_ENTITY_ID_FIELD,\n",
    "    disable_online_serving=False,\n",
    "    worker_count=10,\n",
    "    sync=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor the `Customer` features ingestion job in the console.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go to the [Feature Store Console](https://console.cloud.google.com/vertex-ai/ingestion-jobs) to view your ingestion job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MPhVzBO3t4-"
   },
   "source": [
    "#### Import terminals\n",
    "In the following section, you will import the contents of the events table into your feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1644833483470,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "rLji8AwL3t4_"
   },
   "outputs": [],
   "source": [
    "TERMINAL_ENTITY_ID = \"terminal\"\n",
    "TERMINALS_FEATURES_IDS = [feature.name for feature in terminal_feature_ids.list_features()]\n",
    "TERMINALS_BQ_SOURCE_URI = f\"bq://{TERMINALS_TABLE_NAME}\"\n",
    "TERMINALS_ENTITY_ID_FIELD = \"terminal_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 966684,
     "status": "ok",
     "timestamp": 1644834451377,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "W0ztQUh8pFUD",
    "outputId": "5708fc22-e670-4caf-9a05-64c203fbde17"
   },
   "outputs": [],
   "source": [
    "terminal_entity_type.ingest_from_bq(\n",
    "    feature_ids=TERMINALS_FEATURES_IDS,\n",
    "    feature_time=FEATURE_TIME,\n",
    "    bq_source_uri=TERMINALS_BQ_SOURCE_URI,\n",
    "    entity_id_field=TERMINALS_ENTITY_ID_FIELD,\n",
    "    disable_online_serving=False,\n",
    "    worker_count=10,\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor the ingestion jobs in the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingestion jobs you just created run asynchronously and they should take several minutes to complete. Please monitoring them in the [console](https://console.cloud.google.com/vertex-ai/ingestion-jobs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZzgeozE3t4_"
   },
   "source": [
    "### Search for feature values \n",
    "In this section, you'll run a search query on your feature store to validate that some data was ingested, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1644835516606,
     "user": {
      "displayName": "Ivan Nardini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GirR1vbO3QJ0m2qHiEeorgPnPRSx5bB2uOIfJ4mSQ=s64",
      "userId": "04192340647469915671"
     },
     "user_tz": -60
    },
    "id": "y13EaqMnzibE"
   },
   "outputs": [],
   "source": [
    "customer_aggregated_features = customer_entity_type.read(\n",
    "      entity_ids=[\"5830444124423549\", \"5469689693941771\", \"1361459972478769\"],\n",
    "      feature_ids=CUSTOMERS_FEATURES_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_aggregated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (DO NOT RUN) Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff_feature_store.delete(sync=True, force=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1_data_engineering_sdk.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m96"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
