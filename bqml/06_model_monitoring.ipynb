{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/bigquery_ml/bqml-online-prediction.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/bigquery_ml/bqml-online-prediction.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/bigquery_ml/bqml-online-prediction.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Fraudfinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the Fraudfinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model.\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this notebook, you learn to setup the Vertex AI Model Monitoring service to detect feature skew and drift in the input predict requests.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- Vertex AI Model Monitoring\n",
    "- Vertex AI Prediction\n",
    "- Vertex AI Model resource\n",
    "- Vertex AI Endpoint resource\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Enable model monitoring for Endpoint resource.\n",
    "- Detect skew and drift for feature inputs.\n",
    "- Detect skew and drift for feature attributions.\n",
    "- List, pause, resume and delete monitoring jobs.\n",
    "- Restart monitoring job with predefined input schema.\n",
    "- View logged monitored data\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* BigQuery\n",
    "* BigQuery ML\n",
    "* Vertex AI\n",
    "\n",
    "Learn about [BigQuery Pricing](https://cloud.google.com/bigquery/pricing), [BigQuery ML pricing](https://cloud.google.com/bigquery-ml/pricing), [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Load config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_DATASET = \"tx\"\n",
    "END_DATE_TRAIN = \"2022-01-31\"\n",
    "TRAIN_TABLE_NAME = f\"train_table_{END_DATE_TRAIN.replace('-', '')}\"\n",
    "MODEL_ARTIFACT_URI = f\"gs://{BUCKET_NAME}/deliverables/{MODEL_NAME}\"\n",
    "DEPLOY_VERSION = \"tf2-cpu.2-5\"\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")\n",
    "DEPLOY_MACHINE_TYPE = \"n1-standard-4\"\n",
    "MIN_REPLICA_COUNT = 1\n",
    "MAX_REPLICA_COUNT = 1\n",
    "\n",
    "CUSTOMER_FEATURES = ['customer_id_nb_tx_1day_window',\n",
    "                     'customer_id_nb_tx_60min_window',\n",
    "                     'customer_id_avg_amount_7day_window',\n",
    "                     'customer_id_nb_tx_14day_window',\n",
    "                     'customer_id_avg_amount_30min_window',\n",
    "                     'customer_id_nb_tx_15min_window',\n",
    "                     'customer_id_nb_tx_7day_window',\n",
    "                     'customer_id_avg_amount_15min_window',\n",
    "                     'customer_id_avg_amount_14day_window',\n",
    "                     'customer_id_avg_amount_1day_window',\n",
    "                     'customer_id_avg_amount_60min_window',\n",
    "                     'customer_id_nb_tx_30min_window']\n",
    "\n",
    "TERMINAL_FEATURES = ['terminal_id_risk_7day_window',\n",
    "                     'terminal_id_nb_tx_60min_window',\n",
    "                     'terminal_id_nb_tx_1day_window',\n",
    "                     'terminal_id_nb_tx_15min_window',\n",
    "                     'terminal_id_avg_amount_30min_window',\n",
    "                     'terminal_id_nb_tx_14day_window',\n",
    "                     'terminal_id_risk_14day_window',\n",
    "                     'terminal_id_avg_amount_15min_window',\n",
    "                     'terminal_id_nb_tx_7day_window',\n",
    "                     'terminal_id_nb_tx_30min_window',\n",
    "                     'terminal_id_avg_amount_60min_window',\n",
    "                     'terminal_id_risk_1day_window']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "from typing import Union, List, Dict\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# BigQuery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Vertex AI \n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import model_monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize Vertex AI and BigQuery SDKs for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83859376c893"
   },
   "source": [
    "Create the BigQuery client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ab485806b17"
   },
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f94734ac9312"
   },
   "source": [
    "Use a helper function for sending queries to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to use BigQuery client to run query/job, return job ID or result as DF\n",
    "def run_bq_query(\n",
    "    sql: str, project: str, region: str, return_df=False\n",
    ") -> Union[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    An helper function to run a BigQuery query\n",
    "    Args:\n",
    "        sql: BigQuery query\n",
    "        project: project id\n",
    "        region: region\n",
    "        debug: dry run the query\n",
    "        return_df: return a dataframe or not\n",
    "    Returns:\n",
    "        df: BigQuery query result\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a BigQuery client.\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Try dry run before executing query to catch any errors\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    # Proceed to run query\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client_result = bq_client.query(sql, job_config=job_config)\n",
    "    result = client_result.result()\n",
    "    job_id = client_result.job_id\n",
    "    print(f\"Finished job_id: {job_id}\")\n",
    "\n",
    "    if return_df:\n",
    "        # Get & return data frame\n",
    "        df = result.to_arrow().to_pandas()\n",
    "        return df\n",
    "\n",
    "def get_entity_ids(dataset_uri: str, n_entities:int, entity_name_flag: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    An helper function to get entity ids from the dataset uri\n",
    "    Args:\n",
    "        dataset_uri: The BQ dataset uri\n",
    "        n_entities: number of entities to get\n",
    "        entity_name_flag: The entity name flag\n",
    "    Returns:\n",
    "        entity_ids: list of entity ids\n",
    "    \"\"\"\n",
    "\n",
    "    # Download the table.\n",
    "    table = bigquery.TableReference.from_string(dataset_uri)\n",
    "    rows = bq_client.list_rows(table, max_results=n_entities)\n",
    "\n",
    "    entity_ids = []\n",
    "    for row in rows:\n",
    "        for key, value in row.items():\n",
    "            if key == entity_name_flag:\n",
    "                entity_ids.append(value)\n",
    "            else:\n",
    "                continue\n",
    "    return entity_ids\n",
    "\n",
    "def read_entity_features(entity_name: str, entity_ids: List[str],\n",
    "                         feature_ids: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    An helper function to read entity features from Vertex AI Feature store\n",
    "    Args:\n",
    "        entity_type: Vertex AI Entity type object\n",
    "        entity_ids: list of entity ids\n",
    "        feature_ids: list of feature ids\n",
    "    Returns:\n",
    "        entity_features: dict of entity features\n",
    "    \"\"\"\n",
    "    entity = vertex_ai.EntityType(entity_name)\n",
    "    entity_features = None\n",
    "    try:\n",
    "        entity_features = entity.read(\n",
    "            entity_ids=entity_ids,\n",
    "            feature_ids=feature_ids,\n",
    "        )\n",
    "    except NameError:\n",
    "        print(f\"The entity_ids {', '.join(entity_ids)} does not exist in the feature store\")\n",
    "    else:\n",
    "        entity_features = pd.DataFrame(data=[entity_ids + [0]*len(feature_ids)],\n",
    "                                       columns=[entity.name] + feature_ids)\n",
    "    return entity_features\n",
    "\n",
    "\n",
    "def generate_online_transaction(customer_id: str, terminal_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    An helper function to generate a random sample for online prediction\n",
    "    Args:\n",
    "        customer_id: customer id\n",
    "        terminal_id: terminal id\n",
    "    Returns:\n",
    "        online_sample: Dataframe of online sample\n",
    "    \"\"\"\n",
    "    # TODO - generate a random tx_id\n",
    "    online_tx = {\"tx_id\": \"\",\n",
    "                 \"tx_ts\": datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC'),\n",
    "                 \"customer_id\": customer_id,\n",
    "                 \"terminal_id\": terminal_id,\n",
    "                 \"tx_amount\": round(random.uniform(0, 956.13), 3)\n",
    "                 }\n",
    "    return pd.DataFrame.from_dict([online_tx])\n",
    "\n",
    "def generate_online_sample(customer_features: pd.DataFrame, terminal_features: pd.DataFrame,\n",
    "                           online_transaction: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    An helper function to merge the customer and terminal features with the online transactions\n",
    "    Args:\n",
    "        customer_features: customer features\n",
    "        terminal_features: terminal features\n",
    "        online_transaction: online transactions\n",
    "    Returns:\n",
    "        online_sample: Dataframe of online sample\n",
    "    \"\"\"\n",
    "    online_sample_df = pd.merge(\n",
    "        online_transaction,\n",
    "        customer_features,\n",
    "        left_on=\"customer_id\",\n",
    "        right_on=\"customer\"\n",
    "    )\n",
    "\n",
    "    online_sample_df = pd.merge(\n",
    "        online_sample_df,\n",
    "        terminal_features,\n",
    "        left_on=\"terminal_id\",\n",
    "        right_on=\"terminal\"\n",
    "    )\n",
    "    \n",
    "    online_sample_df = online_sample_df.drop(['tx_id', 'tx_ts', 'customer', 'terminal'], axis=1)\n",
    "\n",
    "    return online_sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your model with Vertex AI Model Monitoring\n",
    "\n",
    "With Vertex AI Model Monitoring, you can monitor for skew and drift detection of the predictions, features and its attributions (Explainable AI) in the incoming prediction requests.\n",
    "\n",
    "With custom models, the model monitoring service requires:\n",
    "\n",
    "- for drift detection, the schema of the features to derive the feature values\n",
    "\n",
    "- for skew detection, a training data sample as baseline to calculate the distribution\n",
    "\n",
    "- for feature attribution skew and drift detection, Vertex Explainable AI to be configured. \n",
    "\n",
    "In the following sections, we are going to cover all those requirements settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Vertex Explainable AI for feature attribution skew and drift detection\n",
    "\n",
    "To configure Vertex Explainable AI for feature attribution skew and drift detection in our case, you need to\n",
    "\n",
    "- Define you explainability specification\n",
    "- Upload the model including the explainability specification\n",
    "\n",
    "Then, you need to pass the explainability specification to the model monitoring job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define you explainability specification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = f\"{PROJECT_ID}.{BQ_DATASET}.{MODEL_NAME}\"\n",
    "bqml_model = bq_client.get_model(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_feature_mapping = []\n",
    "for feature in bqml_model.feature_columns:\n",
    "    index_feature_mapping.append(feature.name)\n",
    "label_name = bqml_model.label_columns[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_params =  vertex_ai.explain.ExplanationParameters({\"sampled_shapley_attribution\": {\"path_count\": 10}})\n",
    "explanation_inputs = {feature_name:{'input_tensor_name':feature_name} for feature_name in index_feature_mapping}\n",
    "explanation_outputs = {label_name: {'output_tensor_name': label_name}}\n",
    "explanation_metadata = vertex_ai.explain.ExplanationMetadata(inputs=explanation_inputs, outputs=explanation_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload the model with explainability specification as new default version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vertex_ai.Model.list(filter=f\"display_name={MODEL_NAME}\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the Vertex AI Model to a Vertex AI Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = vertex_ai.Endpoint.list(filter=f\"display_name={ENDPOINT_NAME}\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=\"fraud_detector_\" + ID,\n",
    "        machine_type=DEPLOY_MACHINE_TYPE,\n",
    "        min_replica_count=MIN_REPLICA_COUNT,\n",
    "        max_replica_count=MAX_REPLICA_COUNT,\n",
    "        # explanation_parameters=explanation_params,\n",
    "        # explanation_metadata=explanation_metadata,\n",
    "        traffic_percentage = 100,\n",
    "        sync=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the feature store\n",
    "ff_feature_store = vertex_ai.Featurestore(featurestore_name=FEATURESTORE_ID)\n",
    "\n",
    "# Get the entity type ids\n",
    "customer_entity_type = [entity.resource_name for entity in ff_feature_store.list_entity_types() if 'customer' in entity.resource_name][0]\n",
    "terminal_entity_type = [entity.resource_name for entity in ff_feature_store.list_entity_types() if 'terminal' in entity.resource_name][0]\n",
    "\n",
    "# Get the entity ids\n",
    "dataset_id = f\"{PROJECT_ID}.{BQ_DATASET}.{TRAIN_TABLE_NAME}\"\n",
    "customer_ids = get_entity_ids(dataset_id, 10, \"entity_type_customer\")\n",
    "terminal_ids = get_entity_ids(dataset_id, 10, \"entity_type_terminal\")\n",
    "\n",
    "# Collect the online transactions\n",
    "online_samples = []\n",
    "for c_id, t_id in zip(customer_ids, terminal_ids):\n",
    "    online_transaction = generate_online_transaction(c_id, t_id)\n",
    "    # Read the customer and terminal entity features\n",
    "    c_features = read_entity_features(customer_entity_type, [c_id], CUSTOMER_FEATURES)\n",
    "    t_features = read_entity_features(terminal_entity_type, [t_id], TERMINAL_FEATURES)\n",
    "    # Merge the customer and terminal features with the online transaction\n",
    "    online_sample = generate_online_sample(\n",
    "        c_features,\n",
    "        t_features,\n",
    "        online_transaction,\n",
    "    )\n",
    "    online_samples.append(online_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the online prediction request\n",
    "try:\n",
    "    for online_sample in online_samples[:5]:\n",
    "        prediction_instance = online_sample.drop([\"customer_id\", \"terminal_id\"], axis=1).to_dict('records')\n",
    "        prediction = endpoint.predict(prediction_instance)\n",
    "        print(\n",
    "            f\"Prediction request: customer_id - {online_sample.customer_id.values} - terminal_id - {online_sample.terminal_id.values} - prediction - {prediction[0][0]['predicted_tx_fraud']} \\n\"\n",
    "        )\n",
    "        time.sleep(1)\n",
    "except Exception as exception:\n",
    "    print(f\"Prediction request: customer_id - {online_sample.customer_id.values} - terminal_id - {online_sample.terminal_id.values} failed.\", \"Exception:\", exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the online prediction request with explaination\n",
    "# for online_sample in online_samples:\n",
    "#     prediction_instance = online_sample.to_dict('records')\n",
    "#     prediction = endpoint.explain(prediction_instance)\n",
    "#     # time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and create a Model Monitoring job\n",
    "\n",
    "To set up either skew detection or drift detection, create a model deployment monitoring job. \n",
    "\n",
    "The job requires the following specifications:\n",
    "\n",
    "- `alert_config`: Configures how alerts are sent to the user. Right now only email alert is supported.\n",
    "- `schedule_config`: Configures model monitoring job scheduling interval in hours. This defines how often the monitoring jobs are triggered.\n",
    "- `logging_sampling_strategy`: Sample Strategy for logging.\n",
    "- `drift_config`\n",
    "- `skew_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the alerting configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emails = ['recipient1@domain.com', 'recipient2@domain.com']\n",
    "alert_config = model_monitoring.EmailAlertConfig(user_emails, enable_logging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the schedule configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_interval = 1\n",
    "schedule_config = model_monitoring.ScheduleConfig(monitor_interval=monitor_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the logging sample strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 0.5 \n",
    "logging_sampling_strategy = model_monitoring.RandomSampleConfig(sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the drift detection configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_threshold_value = 0.05\n",
    "attribution_drift_threshold_value = 0.05\n",
    "\n",
    "drift_thresholds = {\n",
    "    \"tx_amount\": drift_threshold_value,\n",
    "    \"customer_id_nb_tx_1day_window\": drift_threshold_value,\n",
    "    \"customer_id_avg_amount_1day_window\": drift_threshold_value,\n",
    "    \"customer_id_nb_tx_15min_window\": drift_threshold_value,\n",
    "    \"customer_id_avg_amount_15min_window\": drift_threshold_value,\n",
    "    \"terminal_id_nb_tx_1day_window\": drift_threshold_value,\n",
    "    \"terminal_id_risk_1day_window\": drift_threshold_value,\n",
    "    \"terminal_id_nb_tx_15min_window\": drift_threshold_value,\n",
    "    \"terminal_id_avg_amount_15min_window\": drift_threshold_value\n",
    "}\n",
    "\n",
    "attribution_drift_thresholds = {\n",
    "    \"tx_amount\": attribution_drift_threshold_value,\n",
    "    \"customer_id_nb_tx_1day_window\": attribution_drift_threshold_value,\n",
    "    \"customer_id_avg_amount_1day_window\": attribution_drift_threshold_value,\n",
    "    \"customer_id_nb_tx_15min_window\": attribution_drift_threshold_value,\n",
    "    \"customer_id_avg_amount_15min_window\": attribution_drift_threshold_value,\n",
    "    \"terminal_id_nb_tx_1day_window\": attribution_drift_threshold_value,\n",
    "    \"terminal_id_risk_1day_window\": attribution_drift_threshold_value,\n",
    "    \"terminal_id_nb_tx_15min_window\": attribution_drift_threshold_value,\n",
    "    \"terminal_id_avg_amount_15min_window\": attribution_drift_threshold_value\n",
    "}\n",
    "\n",
    "drift_config = model_monitoring.DriftDetectionConfig(\n",
    "    drift_thresholds=drift_thresholds,\n",
    "    attribute_drift_thresholds=attribution_drift_thresholds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the skew detection configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_source_uri = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{TRAIN_TABLE_NAME}\"\n",
    "target = \"tx_fraud\"\n",
    "skew_threshold_value = 0.5\n",
    "attribute_skew_threshold_value = 0.5\n",
    "\n",
    "skew_thresholds = {\n",
    "    \"tx_amount\": skew_threshold_value,\n",
    "    \"customer_id_nb_tx_1day_window\": skew_threshold_value,\n",
    "    \"customer_id_avg_amount_1day_window\": skew_threshold_value,\n",
    "    \"customer_id_nb_tx_15min_window\": skew_threshold_value,\n",
    "    \"customer_id_avg_amount_15min_window\": skew_threshold_value,\n",
    "    \"terminal_id_nb_tx_1day_window\": skew_threshold_value,\n",
    "    \"terminal_id_risk_1day_window\": skew_threshold_value,\n",
    "    \"terminal_id_nb_tx_15min_window\": skew_threshold_value,\n",
    "    \"terminal_id_avg_amount_15min_window\": skew_threshold_value\n",
    "}\n",
    "\n",
    "attribute_skew_thresholds = {\n",
    "    \"tx_amount\": attribute_skew_threshold_value,\n",
    "    \"customer_id_nb_tx_1day_window\": attribute_skew_threshold_value,\n",
    "    \"customer_id_avg_amount_1day_window\": attribute_skew_threshold_value,\n",
    "    \"customer_id_nb_tx_15min_window\": attribute_skew_threshold_value,\n",
    "    \"customer_id_avg_amount_15min_window\": attribute_skew_threshold_value,\n",
    "    \"terminal_id_nb_tx_1day_window\": attribute_skew_threshold_value,\n",
    "    \"terminal_id_risk_1day_window\": attribute_skew_threshold_value,\n",
    "    \"terminal_id_nb_tx_15min_window\": attribute_skew_threshold_value,\n",
    "    \"terminal_id_avg_amount_15min_window\": attribute_skew_threshold_value\n",
    "}\n",
    "\n",
    "skew_config = model_monitoring.SkewDetectionConfig(\n",
    "    data_source=train_data_source_uri,\n",
    "    skew_thresholds=skew_thresholds,\n",
    "    attribute_skew_thresholds=attribute_skew_thresholds,\n",
    "    target_field=target,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the job configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanation_config = model_monitoring.ExplanationConfig()\n",
    "\n",
    "objective_config = model_monitoring.ObjectiveConfig(\n",
    "    skew_detection_config=skew_config,\n",
    "    drift_detection_config=drift_config,\n",
    "    # explanation_config=explanation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the model monitoring job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_job = vertex_ai.ModelDeploymentMonitoringJob.create(\n",
    "    display_name=\"fraud_detection_\" + ID,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    endpoint=endpoint,\n",
    "    logging_sampling_strategy=logging_sampling_strategy,\n",
    "    schedule_config=schedule_config,\n",
    "    alert_config=alert_config,\n",
    "    objective_configs=objective_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the monitoring job state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = monitoring_job.list(filter=f\"display_name=fraud_detection{ID}\")\n",
    "job = jobs[0]\n",
    "print(job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the prediction requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for online_sample in online_samples:\n",
    "        prediction_instance = online_sample.drop([\"customer_id\", \"terminal_id\"], axis=1).to_dict('records')\n",
    "        prediction = endpoint.predict(prediction_instance)\n",
    "        print(\n",
    "            f\"Prediction request: customer_id - {online_sample.customer_id.values} - terminal_id - {online_sample.terminal_id.values} - prediction - {prediction[0][0]['predicted_tx_fraud']} \\n\"\n",
    "        )\n",
    "        time.sleep(1)\n",
    "except Exception as exception:\n",
    "    print(f\"Prediction request: customer_id - {online_sample.customer_id.values} - terminal_id - {online_sample.terminal_id.values} failed.\", \"Exception:\", exception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (DO NOT RUN) Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the monitoring job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitoring_job.pause()\n",
    "# monitoring_job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undeploy the model and delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint.undeploy_all()\n",
    "# endpoint.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bqml-online-prediction.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
