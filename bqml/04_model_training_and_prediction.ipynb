{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Fraudfinder - BigQuery ML - Model training and prediction\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name=Model%20Monitoring&download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmaster%2Fnotebooks%2Fcommunity%2Fmodel_monitoring%2Fmodel_monitoring_feature_attribs.ipynb\">\n",
        "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Google Cloud Notebooks\">Open in Cloud Notebook\n",
        "    </a>\n",
        "  </td> \n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/model_monitoring/model_monitoring_feature_attribs.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/model_monitoring/model_monitoring_feature_attribs.ipynb\">\n",
        "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "[Fraudfinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the Fraudfinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66bbec2a5e37"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this notebook, Using the data in Vertex AI Feature Store, that you previously ingested data into, you will train a model using BigQuery ML, register the model to Vertex AI Model Registry, and deploy it to an endpoint for real-time prediction. \n",
        "\n",
        "In this tutorial, you will learn how to:\n",
        "\n",
        "- Train a logistic regression model in BigQuery using BigQueryML\n",
        "- Evaluate the model\n",
        "- Test a prediction \n",
        "- Deploy to an endpoint on Vertex AI\n",
        "- Make an online prediction \n",
        "\n",
        "\n",
        "This tutorial uses the following Google Cloud data analytics and ML services:\n",
        "\n",
        "- BigQuery\n",
        "- BigQuery ML\n",
        "- Vertex AI Model Registry\n",
        "- Vertex endpoints\n",
        "\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Using Python & SQL to query the public data in BigQuery\n",
        "- Preparing the data for modeling\n",
        "- Training a classification model using BigQuery ML and registering it to Vertex AI Model Registry\n",
        "- Inspecting the model on Vertex AI Model Registry\n",
        "- Deploying the model to an endpoint on Vertex AI\n",
        "- Making sample online predictions to the model endpoint\n",
        "\n",
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* BigQuery\n",
        "* BigQuery ML\n",
        "* Vertex AI\n",
        "\n",
        "\n",
        "Learn about [BigQuery Pricing](https://cloud.google.com/bigquery/pricing), [BigQuery ML pricing](https://cloud.google.com/bigquery-ml/pricing), [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze4-nDLfK4pw"
      },
      "source": [
        "### Load config settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCuSR8GkAgzl"
      },
      "outputs": [],
      "source": [
        "GCP_PROJECTS = !gcloud config get-value project\n",
        "PROJECT_ID = GCP_PROJECTS[0]\n",
        "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
        "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
        "print(config.n)\n",
        "exec(config.n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39705682a1a0"
      },
      "source": [
        "###Â Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c302a15cbe2"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3acb3af78aa"
      },
      "outputs": [],
      "source": [
        "start_date = datetime.now() - timedelta(days=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dda2792a08c"
      },
      "outputs": [],
      "source": [
        "START_DATE_TRAIN = (\n",
        "    \"2022-01-31\"  # consider few days for training (demo) # it should be yesterday\n",
        ")\n",
        "END_DATE_TRAIN = \"2022-01-31\"\n",
        "CUSTOMERS_TABLE_NAME = f\"customers_{END_DATE_TRAIN.replace('-', '')}\"\n",
        "TERMINALS_TABLE_NAME = f\"terminals_{END_DATE_TRAIN.replace('-', '')}\"\n",
        "\n",
        "SERVING_FEATURE_IDS = {\"customer\": [\"*\"], \"terminal\": [\"*\"]}\n",
        "READ_INSTANCES_TABLE = f\"ground_truth_{END_DATE_TRAIN.replace('-', '')}\"\n",
        "READ_INSTANCES_URI = f\"bq://{PROJECT_ID}.tx.{READ_INSTANCES_TABLE}\"\n",
        "BQ_TABLE_NAME = f\"train_table_{END_DATE_TRAIN.replace('-', '')}\"\n",
        "TRAIN_TABLE_URI = f\"bq://{PROJECT_ID}.tx.{BQ_TABLE_NAME}\"\n",
        "ff_public_topic_id = \"projects/cymbal-fraudfinder/topics/ff-tx\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5d6e49cc474"
      },
      "source": [
        "#### Payload schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fd429262fab"
      },
      "outputs": [],
      "source": [
        "PAYLOAD_SCHEMA = {\n",
        "    \"tx_amount\": \"float64\",\n",
        "    \"customer_id_nb_tx_1day_window\": \"int64\",\n",
        "    \"customer_id_nb_tx_7day_window\": \"int64\",\n",
        "    \"customer_id_nb_tx_14day_window\": \"int64\",\n",
        "    \"customer_id_avg_amount_1day_window\": \"float64\",\n",
        "    \"customer_id_avg_amount_7day_window\": \"float64\",\n",
        "    \"customer_id_avg_amount_14day_window\": \"float64\",\n",
        "    \"customer_id_nb_tx_15min_window\": \"int64\",\n",
        "    \"customer_id_avg_amount_15min_window\": \"float64\",\n",
        "    \"customer_id_nb_tx_30min_window\": \"int64\",\n",
        "    \"customer_id_avg_amount_30min_window\": \"float64\",\n",
        "    \"customer_id_nb_tx_60min_window\": \"int64\",\n",
        "    \"customer_id_avg_amount_60min_window\": \"float64\",\n",
        "    \"terminal_id_nb_tx_1day_window\": \"int64\",\n",
        "    \"terminal_id_nb_tx_7day_window\": \"int64\",\n",
        "    \"terminal_id_nb_tx_14day_window\": \"int64\",\n",
        "    \"terminal_id_risk_1day_window\": \"float64\",\n",
        "    \"terminal_id_risk_7day_window\": \"float64\",\n",
        "    \"terminal_id_risk_14day_window\": \"float64\",\n",
        "    \"terminal_id_nb_tx_15min_window\": \"int64\",\n",
        "    \"terminal_id_avg_amount_15min_window\": \"float64\",\n",
        "    \"terminal_id_nb_tx_30min_window\": \"int64\",\n",
        "    \"terminal_id_avg_amount_30min_window\": \"float64\",\n",
        "    \"terminal_id_nb_tx_60min_window\": \"int64\",\n",
        "    \"terminal_id_avg_amount_60min_window\": \"float64\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "170fc7eb629a"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "import pandas as pd\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from google.cloud import bigquery\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI and BigQuery SDKs for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "outputs": [],
      "source": [
        "vertex_ai.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5faea0ea0937"
      },
      "source": [
        "### Helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f94734ac9312"
      },
      "source": [
        "Use a helper function for sending queries to BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e364dab1d353"
      },
      "outputs": [],
      "source": [
        "# Wrapper to use BigQuery client to run query/job, return job ID or result as DF\n",
        "def run_bq_query(sql: str) -> Union[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Input: SQL query, as a string, to execute in BigQuery\n",
        "    Returns the query results as a pandas DataFrame, or error, if any\n",
        "    \"\"\"\n",
        "\n",
        "    bq_client = bigquery.Client()\n",
        "\n",
        "    # Try dry run before executing query to catch any errors\n",
        "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
        "    bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    # If dry run succeeds without errors, proceed to run query\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    client_result = bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    job_id = client_result.job_id\n",
        "\n",
        "    # Wait for query/job to finish running. then get & return data frame\n",
        "    df = client_result.result().to_arrow().to_pandas()\n",
        "    print(f\"Finished job_id: {job_id}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59b4103c0263"
      },
      "source": [
        "## Fetching feature values for model training\n",
        "\n",
        "Let's assume now that new labels become available. The features you calculated are not align with them. In your case you will know if a transaction is a fraud or not only after some time the investigation has run and validated your predictions. Because of all that, it sounds incredibly difficult to say which set of features contains the most up to date historical information associated with the label you want to predict. And, when you are not able to guarantee that, the performance of your model would be badly affected because you serve no representative features of the data and the label from the field when the model goes live. So you need a way to get the most updated features you calculated over time before the label becomes available. In other terms, you need a time travel machine for your features with respect to your labels! \n",
        "\n",
        "Vertex AI Feature store allows to address this challenge by providing point-in-time lookups whose fetch the most up to date features with respect of the time label becomes available \n",
        "\n",
        "To fetch training data, we have to specify the following inputs to batch serving:\n",
        "\n",
        "- a file containing a \"query\", with the entities and timestamps for each label\n",
        "- a list of features to fetch values for\n",
        "- the destination location and format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "232ca3c3be7e"
      },
      "source": [
        "### Read-instance list\n",
        "\n",
        "In our case, we need a csv file with content formatted like the table below:\n",
        "\n",
        "|customer                     |terminal|timestamp                                    |\n",
        "|-----------------------------|--------|---------------------------------------------|\n",
        "|xxx3859                         |xxx8811    |2021-07-07 00:01:10 UTC                      |\n",
        "|xxx4165                         |xxx8810    |2021-07-07 00:01:55 UTC                      |\n",
        "|xxx2289                         |xxx2081    |2021-07-07 00:02:12 UTC                      |\n",
        "|xxx3227                         |xxx3011    |2021-07-07 00:03:23 UTC                      |\n",
        "|xxx2819                         |xxx6263    |2021-07-07 00:05:30 UTC                      |\n",
        "\n",
        "where the column names are the name of entities in Feature Store and the timestamps represents the time an event occurred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "056bbbc7c579"
      },
      "outputs": [],
      "source": [
        "read_instances_query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE {PROJECT_ID}.tx.{READ_INSTANCES_TABLE} as (\n",
        "    SELECT\n",
        "        raw_tx.TX_TS AS timestamp,\n",
        "        raw_tx.CUSTOMER_ID AS customer,\n",
        "        raw_tx.TERMINAL_ID AS terminal,\n",
        "        raw_tx.TX_AMOUNT AS tx_amount,\n",
        "        raw_lb.TX_FRAUD AS tx_fraud,\n",
        "    FROM \n",
        "        tx.tx as raw_tx\n",
        "    LEFT JOIN \n",
        "        tx.txlabels as raw_lb\n",
        "    ON raw_tx.TX_ID = raw_lb.TX_ID\n",
        "    WHERE\n",
        "        DATE(raw_tx.TX_TS) = \"2022-01-31\" -- this should be the duration of the period that we backfilled for\n",
        ");\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "run_bq_query(read_instances_query)\n",
        "run_bq_query(f\"SELECT * FROM {PROJECT_ID}.tx.{READ_INSTANCES_TABLE} LIMIT 10\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9539737e33ec"
      },
      "source": [
        "### Get Feature Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1a98b7681b9"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    ff_feature_store = vertex_ai.Featurestore(f\"{FEATURESTORE_ID}\")\n",
        "    print(f\"\"\"The feature store {FEATURESTORE_ID} found!\"\"\")\n",
        "except NameError:\n",
        "    print(f\"\"\"The feature store {FEATURESTORE_ID} does not exist!\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efb7b335cafd"
      },
      "source": [
        "### Export a sample of data to a Bigquery using Feature store point in time capabilities. \n",
        "\n",
        "In this section, we will use Batch Serving of feature store to prepare a dataset for training by calling the BatchReadFeatureValues API. Batch Serving is used to fetch a large set of feature values with high throughput, typically for training a model or batch prediction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2a940d39f47"
      },
      "outputs": [],
      "source": [
        "ff_feature_store.batch_serve_to_bq(\n",
        "    bq_destination_output_uri=TRAIN_TABLE_URI,\n",
        "    serving_feature_ids=SERVING_FEATURE_IDS,\n",
        "    read_instances_uri=READ_INSTANCES_URI,\n",
        "    pass_through_fields=[\"tx_amount\", \"tx_fraud\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13b6ce9f8d8b"
      },
      "source": [
        "### Inspect the feature table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49dd00d5fbe5"
      },
      "source": [
        "As seen below, each row represents a transaction id, and the columns represent the attributes of the transaction (i.e customer_id, terminal_id, tx_amount),  aggregated behavioral features and the label (whether the transaction is fraudulent or not)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1789398c4571"
      },
      "outputs": [],
      "source": [
        "sql_inspect = f\"\"\"\n",
        "SELECT\n",
        "    *\n",
        "FROM\n",
        "    `tx.{BQ_TABLE_NAME}`\n",
        "LIMIT\n",
        "    5\n",
        "\"\"\"\n",
        "run_bq_query(sql_inspect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4a686de97f5"
      },
      "source": [
        "## BigQuery ML introduction\n",
        "\n",
        "BigQuery ML (BQML) provides the capability to train ML tabular models, such as classification, regression, forecasting, and matrix factorization, in BigQuery using SQL syntax directly. BigQuery ML uses the scalable infrastructure of BigQuery ML so you don't need to set up additional infrastructure for training or batch serving.\n",
        "\n",
        "Learn more about [BigQuery ML documentation](https://cloud.google.com/bigquery-ml/docs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02f304053600"
      },
      "source": [
        "### Train a logistic regression model using BigQuery ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "566f3395f20b"
      },
      "source": [
        "The query below trains a logistic regression model using BigQuery ML. BigQuery resources are used to train the model.\n",
        "\n",
        "In the `OPTIONS` parameter:\n",
        "* with `model_registry=\"vertex_ai\"`, the BigQuery ML model will automatically be [registered to Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/model-registry-bqml), which enables you to view all of your registered models and its versions on Google Cloud in one place.\n",
        "\n",
        "* `vertex_ai_model_version_aliases allows you to set aliases to help you keep track of your model version ([documentation](https://cloud.google.com/vertex-ai/docs/model-registry/model-alias))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "414c45011c1f"
      },
      "outputs": [],
      "source": [
        "# this cell may take ~4 min to run\n",
        "\n",
        "sql_train_model_bqml = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `tx.{MODEL_NAME}` \n",
        "OPTIONS(\n",
        "  MODEL_TYPE=\"LOGISTIC_REG\",\n",
        "  INPUT_LABEL_COLS=[\"tx_fraud\"],\n",
        "  EARLY_STOP =TRUE,\n",
        "  MIN_REL_PROGRESS=0.01,\n",
        "  model_registry=\"vertex_ai\", \n",
        "  vertex_ai_model_version_aliases=['bqml-ff', 'v1']\n",
        ") AS\n",
        "\n",
        "SELECT\n",
        "  * EXCEPT(timestamp, entity_type_customer, entity_type_terminal)\n",
        "FROM\n",
        "   `tx.{BQ_TABLE_NAME}`\n",
        "\"\"\"\n",
        "\n",
        "print(sql_train_model_bqml)\n",
        "\n",
        "run_bq_query(sql_train_model_bqml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92f26cdecf4d"
      },
      "source": [
        "#### Inspect the model on Vertex AI Model Registry\n",
        "When the model was trained in BigQuery ML, the line `model_registry=\"vertex_ai\"` registered the model to Vertex AI Model Registry automatically upon completion.\n",
        "\n",
        "You can view the model on the <a href=\"https://console.cloud.google.com/vertex-ai/models\" target=\"_blank\">Vertex AI Model Registry page</a>, or use the code below to check that it was successfully registered:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e179df915a9"
      },
      "outputs": [],
      "source": [
        "model = vertex_ai.Model(model_name=MODEL_NAME)\n",
        "\n",
        "print(model.gca_resource)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a90e98c72a05"
      },
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aaaae772f67"
      },
      "source": [
        "With the model created, you can now evaluate the logistic regression model. Behind the scenes, BigQuery ML automatically [split the data](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create#data_split_method), which makes it easier to quickly train and evaluate models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1f8ac93d570"
      },
      "outputs": [],
      "source": [
        "sql_evaluate_model = f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.EVALUATE(MODEL tx.{MODEL_NAME})\n",
        "\"\"\"\n",
        "\n",
        "print(sql_evaluate_model)\n",
        "\n",
        "run_bq_query(sql_evaluate_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9f807a50f38"
      },
      "source": [
        "These metrics help you understand the performance of the model. \n",
        "\n",
        "There are various metrics for logistic regression and other model types (full list of metrics can be found in the [documentation](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate#mlevaluate_output))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e806ebc48a2"
      },
      "source": [
        "### Batch prediction (with Explainable AI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d31605829283"
      },
      "source": [
        "Make a batch prediction in BigQuery ML on the original training data to check the probability of a transaction to be fraudulent for transaction, as seen in the `probability` column, with the predicted label under the `predicted_tx_fraud` column.\n",
        "\n",
        "[ML.EXPLAIN_PREDICT](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-explain-predict) has built-in [Explainable AI](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-xai-overview). This allows you to see the top contributing features to each prediction and interpret how it was computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d500fdbfb44"
      },
      "outputs": [],
      "source": [
        "sql_explain_predict = f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.EXPLAIN_PREDICT(MODEL tx.{MODEL_NAME},\n",
        "    (SELECT * FROM  `tx.{BQ_TABLE_NAME}` LIMIT 10)\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "print(sql_explain_predict)\n",
        "\n",
        "run_bq_query(sql_explain_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa1f96c0f452"
      },
      "source": [
        "Since the `top_feature_attributions` is a nested column, you can unnest the array ([documentation](https://cloud.google.com/bigquery/docs/reference/standard-sql/arrays)) into separate rows for each of the features. In other words, since ML.EXPLAIN_PREDICT provides the top 5 most important features, using `UNNEST` results in 5 rows per prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "278b3441084b"
      },
      "outputs": [],
      "source": [
        "sql_explain_predict = f\"\"\"\n",
        "SELECT\n",
        "  tfa.*,\n",
        "  predicted_tx_fraud,\n",
        "  probability,\n",
        "  baseline_prediction_value,\n",
        "  prediction_value,\n",
        "  approximation_error,\n",
        "FROM\n",
        "  ML.EXPLAIN_PREDICT(MODEL tx.{MODEL_NAME},\n",
        "    (SELECT * FROM `tx.{BQ_TABLE_NAME}` )\n",
        "    ),\n",
        "  UNNEST(top_feature_attributions) as tfa\n",
        "\"\"\"\n",
        "\n",
        "print(sql_explain_predict)\n",
        "\n",
        "run_bq_query(sql_explain_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89455f708f54"
      },
      "source": [
        "### Deploy the model to an endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6120dcc1ff6"
      },
      "source": [
        "While BigQuery ML supports batch prediction with [ML.PREDICT](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict) and [ML.EXPLAIN_PREDICT](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-explain-predict), BigQuery ML is not suitable for real-time predictions where you need low latency predictions with potentially high frequency of requests.\n",
        "\n",
        "In other words, deploying the BigQuery ML model to an endpoint enables you to do online predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1ab7e1ac83c"
      },
      "source": [
        "#### Create a Vertex AI endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a61ea55f685"
      },
      "source": [
        "To deploy your model to an endpoint, you will first need to create an endpoint before you deploy the model to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ce73125dff6"
      },
      "outputs": [],
      "source": [
        "endpoint = vertex_ai.Endpoint.create(\n",
        "    display_name=ENDPOINT_NAME,\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")\n",
        "\n",
        "print(endpoint.display_name)\n",
        "print(endpoint.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b58a104207d2"
      },
      "source": [
        "#### List endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "951ed1693f6b"
      },
      "source": [
        "List the endpoints to make sure it has successfully been created. You can also view your endpoints on the [Vertex AI Endpoints page](https://console.cloud.google.com/vertex-ai/endpoints)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "887e007ee661"
      },
      "outputs": [],
      "source": [
        "endpoint.list(order_by=\"update_time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba0d40b26cfb"
      },
      "source": [
        "#### Deploy model to Vertex endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a90be5b77a2"
      },
      "source": [
        "With the model, you can now deploy it to an endpoint. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a54ceab19a6e"
      },
      "outputs": [],
      "source": [
        "# deploying the model to the endpoint may take 10-15 minutes\n",
        "model.deploy(endpoint=endpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c303d779477b"
      },
      "source": [
        "You can also check on the status of your model by visiting the [Vertex AI Endpoints page](https://console.cloud.google.com/vertex-ai/endpoints)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cb04d49c6f1"
      },
      "source": [
        "### Make online predictions to the endpoint with pub/sub -> pull subscription -> notebook -> Vertex AI endpoint\n",
        "Using a sample of the training data, you can test the endpoint to make online predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2197d74f6e4"
      },
      "source": [
        "Below are some helper functions to help make it easier to read streaming data and do online predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54135f8399af"
      },
      "outputs": [],
      "source": [
        "# A function to read some sample messages (transaction) from the public Pub/Sub\n",
        "def read_from_sub(project_id, subscription_path, messages=10):\n",
        "    import ast\n",
        "\n",
        "    from google.api_core import retry\n",
        "    from google.cloud import pubsub_v1\n",
        "\n",
        "    subscriber = pubsub_v1.SubscriberClient()\n",
        "    subscription_path = subscriber.subscription_path(project_id, subscription_path)\n",
        "\n",
        "    # Wrap the subscriber in a 'with' block to automatically call close() to\n",
        "    # close the underlying gRPC channel when done.\n",
        "    with subscriber:\n",
        "        # The subscriber pulls a specific number of messages. The actual\n",
        "        # number of messages pulled may be smaller than max_messages.\n",
        "        response = subscriber.pull(\n",
        "            subscription=subscription_path,\n",
        "            max_messages=messages,\n",
        "            retry=retry.Retry(deadline=300),\n",
        "        )\n",
        "\n",
        "        if len(response.received_messages) == 0:\n",
        "            print(\"no messages\")\n",
        "            return\n",
        "\n",
        "        ack_ids = []\n",
        "        msg_data = []\n",
        "        for received_message in response.received_messages:\n",
        "            msg = ast.literal_eval(received_message.message.data.decode(\"utf-8\"))\n",
        "            print(f\"Received: {msg}.\")\n",
        "            msg_data.append(msg)\n",
        "            ack_ids.append(received_message.ack_id)\n",
        "\n",
        "        # Acknowledges the received messages so they will not be sent again.\n",
        "        subscriber.acknowledge(subscription=subscription_path, ack_ids=ack_ids)\n",
        "\n",
        "        print(\n",
        "            f\"Received and acknowledged {len(response.received_messages)} messages from {subscription_path}.\"\n",
        "        )\n",
        "\n",
        "        return msg_data\n",
        "\n",
        "\n",
        "# A function for pre-processing of payload before sending it to a Vertex AI endpoint\n",
        "def preprocess(payload):\n",
        "    # replace NaN's\n",
        "    for key, value in payload.items():\n",
        "        if value is None:\n",
        "            payload[key] = 0.0\n",
        "    return payload\n",
        "\n",
        "\n",
        "# A function to lookup features in Vertex AI Feature Store\n",
        "def features_lookup(ff_feature_store, entity, entity_ids):\n",
        "    entity_type = ff_feature_store.get_entity_type(entity)\n",
        "    aggregated_features = entity_type.read(entity_ids=entity_ids, feature_ids=\"*\")\n",
        "    aggregated_features_preprocessed = preprocess(aggregated_features)\n",
        "    features = aggregated_features_preprocessed.iloc[0].to_dict()\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f84067f6fe1f"
      },
      "source": [
        "You can now read some messages from Pub/Sub, preprocess it, augment it with some features from Vertex AI Feature Store, and submit to Vertex AI Endpoint for online predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f41ab5d61080"
      },
      "outputs": [],
      "source": [
        "messages = read_from_sub(\n",
        "    project_id=PROJECT_ID, subscription_path=\"ff-tx-sub\", messages=10\n",
        ")\n",
        "\n",
        "for payload_input in messages:\n",
        "    print(f\"The recieved payload from Pub/Sub is: {payload_input}\")\n",
        "    print(f\"-----------------------------------------\")\n",
        "    payload = {}\n",
        "    payload[\"tx_amount\"] = payload_input[\"TX_AMOUNT\"]\n",
        "    # look up the customer featues from FS (wrttie by batch ingestion daily and by Dataflow in real-time)\n",
        "    customer_features = features_lookup(\n",
        "        ff_feature_store, \"customer\", [payload_input[\"CUSTOMER_ID\"]]\n",
        "    )\n",
        "    payload[\"customer_id_nb_tx_1day_window\"] = customer_features[\n",
        "        \"customer_id_nb_tx_1day_window\"\n",
        "    ]\n",
        "    payload[\"customer_id_nb_tx_7day_window\"] = customer_features[\n",
        "        \"customer_id_nb_tx_7day_window\"\n",
        "    ]\n",
        "    payload[\"customer_id_nb_tx_14day_window\"] = customer_features[\n",
        "        \"customer_id_nb_tx_14day_window\"\n",
        "    ]\n",
        "    payload[\"customer_id_avg_amount_1day_window\"] = customer_features[\n",
        "        \"customer_id_avg_amount_1day_window\"\n",
        "    ]\n",
        "    payload[\"customer_id_avg_amount_7day_window\"] = customer_features[\n",
        "        \"customer_id_avg_amount_7day_window\"\n",
        "    ]\n",
        "    payload[\"customer_id_avg_amount_14day_window\"] = customer_features[\n",
        "        \"customer_id_avg_amount_14day_window\"\n",
        "    ]\n",
        "    payload[\"customer_id_nb_tx_15min_window\"] = customer_features[\n",
        "        \"customer_id_nb_tx_15min_window\"\n",
        "    ]\n",
        "    payload[\"customer_id_avg_amount_15min_window\"] = customer_features[\n",
        "        \"customer_id_avg_amount_15min_window\"\n",
        "    ]\n",
        "    payload[\"customer_id_nb_tx_30min_window\"] = customer_features[\n",
        "        \"customer_id_nb_tx_30min_window\"\n",
        "    ]\n",
        "    payload[\"customer_id_avg_amount_30min_window\"] = customer_features[\n",
        "        \"customer_id_avg_amount_30min_window\"\n",
        "    ]\n",
        "    payload[\"customer_id_nb_tx_60min_window\"] = customer_features[\n",
        "        \"customer_id_nb_tx_60min_window\"\n",
        "    ]\n",
        "    payload[\"customer_id_avg_amount_60min_window\"] = customer_features[\n",
        "        \"customer_id_avg_amount_60min_window\"\n",
        "    ]\n",
        "    # look up the terminal featues from FS (wrttie by batch ingestion daily and by Dataflow in real-time)\n",
        "    terminal_features = features_lookup(\n",
        "        ff_feature_store, \"terminal\", [payload_input[\"TERMINAL_ID\"]]\n",
        "    )\n",
        "    payload[\"terminal_id_nb_tx_1day_window\"] = terminal_features[\n",
        "        \"terminal_id_nb_tx_1day_window\"\n",
        "    ]\n",
        "    payload[\"terminal_id_nb_tx_7day_window\"] = terminal_features[\n",
        "        \"terminal_id_nb_tx_7day_window\"\n",
        "    ]\n",
        "    payload[\"terminal_id_nb_tx_14day_window\"] = terminal_features[\n",
        "        \"terminal_id_nb_tx_14day_window\"\n",
        "    ]\n",
        "    payload[\"terminal_id_risk_1day_window\"] = terminal_features[\n",
        "        \"terminal_id_risk_1day_window\"\n",
        "    ]\n",
        "    payload[\"terminal_id_risk_7day_window\"] = terminal_features[\n",
        "        \"terminal_id_risk_7day_window\"\n",
        "    ]\n",
        "    payload[\"terminal_id_risk_14day_window\"] = terminal_features[\n",
        "        \"terminal_id_risk_14day_window\"\n",
        "    ]\n",
        "    payload[\"terminal_id_nb_tx_15min_window\"] = terminal_features[\n",
        "        \"terminal_id_nb_tx_15min_window\"\n",
        "    ]\n",
        "    payload[\"terminal_id_avg_amount_15min_window\"] = terminal_features[\n",
        "        \"terminal_id_avg_amount_15min_window\"\n",
        "    ]\n",
        "    payload[\"terminal_id_nb_tx_30min_window\"] = terminal_features[\n",
        "        \"terminal_id_nb_tx_30min_window\"\n",
        "    ]\n",
        "    payload[\"terminal_id_avg_amount_30min_window\"] = terminal_features[\n",
        "        \"terminal_id_avg_amount_30min_window\"\n",
        "    ]\n",
        "    payload[\"terminal_id_nb_tx_60min_window\"] = terminal_features[\n",
        "        \"terminal_id_nb_tx_60min_window\"\n",
        "    ]\n",
        "    payload[\"terminal_id_avg_amount_60min_window\"] = terminal_features[\n",
        "        \"terminal_id_avg_amount_60min_window\"\n",
        "    ]\n",
        "    payload = preprocess(payload)\n",
        "    print(f\"The input payload to the Vertex AI endpoint: {payload}\")\n",
        "    print(f\"-----------------------------------------\")\n",
        "\n",
        "    result = endpoint.predict(instances=[payload])\n",
        "    print(f\"The prediction result: {result}\")\n",
        "    print(f\"===============================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "094c24798d33"
      },
      "source": [
        "## (DO NOT RUN) Cleaning up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5191374613a"
      },
      "outputs": [],
      "source": [
        "# endpoint[-1].undeploy_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75cb7aba1566"
      },
      "outputs": [],
      "source": [
        "# delete_model_sql = f\"\"\"\n",
        "# DROP MODEL `{PROJECT_ID}.{BQ_DATASET}.{BQML_MODEL_NAME}`\n",
        "# \"\"\"\n",
        "\n",
        "# bq_query(delete_model_sql)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "04_model_training_and_prediction.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
