{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Fraudfinder - Feature engineering (streaming)\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name=Model%20Monitoring&download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmaster%2Fnotebooks%2Fcommunity%2Fmodel_monitoring%2Fmodel_monitoring_feature_attribs.ipynb\">\n",
    "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Google Cloud Notebooks\">Open in Cloud Notebook\n",
    "    </a>\n",
    "  </td> \n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/model_monitoring/model_monitoring_feature_attribs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/model_monitoring/model_monitoring_feature_attribs.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Fraudfinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the Fraudfinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "In order to calculate very recent customer and terminal activity (i.e. within the last hour), computation has to be done on real-time streaming data, rather than via batch-based feature engineering.\n",
    "\n",
    "This notebook shows a step-by-step guide to create real-time data pipelines in order to:\n",
    "\n",
    "- calculate customer spending features (last 15-mins, 30-mins, and 60-mins)\n",
    "- calculate terminal activity features (last 15-mins, 30-mins, and 60-mins)\n",
    "\n",
    "by pulling the streaming data from a Pub/Sub topic and ingesting the features directly into Vertex AI Feature Store using Dataflow. \n",
    "\n",
    "In the following notebook, you will learn to:\n",
    "\n",
    "- Create features, using window and aggreation functions in an Apache Beam pipeline\n",
    "- Deploy the Apache Beam pipeline to Dataflow\n",
    "- Ingest engineered features to Vertex AI Feature Store\n",
    "\n",
    "Note: As deploying Apache Beam pipelines to Dataflow works better if we submit the job from a Python Script, we will be writting the code into a python script instead of running directly on the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf",
    "tags": []
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Importing all the required libraries for the latter part of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘beam_pipeline/’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir beam_pipeline/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting beam_pipeline/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile beam_pipeline/main.py\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "from typing import Tuple, Any, List\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.transforms.combiners import CountCombineFn, MeanCombineFn\n",
    "    \n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import aiplatform_v1beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "We need to set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an auxiliary magic function\n",
    "\n",
    "The magic function `writefile` from Jupyter Notebook can only write the cell as is and could not unpack Python variables. Hence, we need to create an auxiliary magic function that can unpack Python variables and write them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'a') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the variables to a Python script with the new magic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_variables = \"\\n\".join(config[1:-1])\n",
    "project_variables += f'\\nPROJECT_ID = \"{PROJECT}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate beam_pipeline/main.py\n",
    "\n",
    "# Project variables\n",
    "{project_variables}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constant variables\n",
    "\n",
    "Constanct variables are defined here to be used in the later part of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to beam_pipeline/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a beam_pipeline/main.py\n",
    "\n",
    "# Pub/Sub variables\n",
    "SUBSCRIPTION_NAME = \"ff-txlabels-sub\"\n",
    "SUBSCRIPTION_PATH = f\"projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_NAME}\"\n",
    "\n",
    "# Dataflow variables\n",
    "MINUTES_WINDOWS = [15, 30, 60]\n",
    "SECONDS_WINDOWS = [x * 60 for x in sorted(MINUTES_WINDOWS)]  # convert to seconds\n",
    "WINDOW_SIZE = SECONDS_WINDOWS[-1]  # the largest window\n",
    "WINDOW_PERIOD = 1 * 60  # 1 minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining auxiliary functions and classes\n",
    "\n",
    "Here we define auxiliary functions and classes that will be used in building our real-time feature engineering and ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to beam_pipeline/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a beam_pipeline/main.py\n",
    "def to_unix_time(time_str: str, time_format='%Y-%m-%d %H:%M:%S') -> int:\n",
    "    import time\n",
    "    \"\"\"Converts a time string into Unix time.\"\"\"\n",
    "    time_tuple = time.strptime(time_str, time_format)\n",
    "    return int(time.mktime(time_tuple))\n",
    "    \n",
    "    \n",
    "class AddAddtionalInfo(beam.DoFn):\n",
    "    \"\"\"Add composite key and difference from window end timestamp to element\"\"\"\n",
    "    def process(self, element: Tuple, timestamp=beam.DoFn.TimestampParam, window=beam.DoFn.WindowParam) -> Tuple:\n",
    "        window_end_dt = window.end.to_utc_datetime().strftime(\"%Y%m%d%H%M%S\")\n",
    "        new_element = {\n",
    "            'TX_ID': element['TX_ID'],\n",
    "            'TX_TS': element['TX_TS'],                                                                                                                                                                                                                                      \n",
    "            'CUSTOMER_ID': element['CUSTOMER_ID'],\n",
    "            'TERMINAL_ID': element['TERMINAL_ID'],\n",
    "            'TX_AMOUNT': element['TX_AMOUNT'],\n",
    "            'TX_FRAUD': element['TX_FRAUD'],\n",
    "            'CUSTOMER_ID_COMPOSITE_KEY': f\"{element['CUSTOMER_ID']}_{window_end_dt}\",\n",
    "            'TERMINAL_ID_COMPOSITE_KEY': f\"{element['TERMINAL_ID']}_{window_end_dt}\",\n",
    "            'TS_DIFF': window.end - timestamp\n",
    "        }\n",
    "        return (new_element,)\n",
    "\n",
    "    \n",
    "class WriteFeatures(beam.DoFn):\n",
    "    def __init__(self, resource_name: str):\n",
    "        self.resource_name = resource_name\n",
    "    \n",
    "    def populate_customer_payload(self, new_records, aggregated) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Pepare payloads for customer related features to be written\n",
    "        at the vertex feature store. The values are required to be of FeatureValue type.\n",
    "        \"\"\"\n",
    "        payloads = []\n",
    "        for row in new_records:\n",
    "            payload = aiplatform_v1beta1.WriteFeatureValuesPayload()\n",
    "            payload.entity_id = row.CUSTOMER_ID\n",
    "            payload.feature_values = {\n",
    "                \"customer_id_nb_tx_15min_window\": aiplatform_v1beta1.FeatureValue(int64_value=aggregated.CUSTOMER_ID_NB_TX_15MIN_WINDOW),\n",
    "                \"customer_id_nb_tx_30min_window\": aiplatform_v1beta1.FeatureValue(int64_value=aggregated.CUSTOMER_ID_NB_TX_30MIN_WINDOW),\n",
    "                \"customer_id_nb_tx_60min_window\": aiplatform_v1beta1.FeatureValue(int64_value=aggregated.CUSTOMER_ID_NB_TX_60MIN_WINDOW),\n",
    "                \"customer_id_avg_amount_15min_window\": aiplatform_v1beta1.FeatureValue(double_value=aggregated.CUSTOMER_ID_SUM_AMOUNT_15MIN_WINDOW / aggregated.CUSTOMER_ID_NB_TX_15MIN_WINDOW),\n",
    "                \"customer_id_avg_amount_30min_window\": aiplatform_v1beta1.FeatureValue(double_value=aggregated.CUSTOMER_ID_SUM_AMOUNT_30MIN_WINDOW / aggregated.CUSTOMER_ID_NB_TX_30MIN_WINDOW),\n",
    "                \"customer_id_avg_amount_60min_window\": aiplatform_v1beta1.FeatureValue(double_value=aggregated.CUSTOMER_ID_AVG_AMOUNT_60MIN_WINDOW),\n",
    "            }\n",
    "            payloads.append(payload)\n",
    "        return payloads\n",
    "    \n",
    "    def populate_terminal_payload(self, new_records, aggregated) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Prepare payloads for terminal related features to be written\n",
    "        at the vertex feature store. The values are required to be of FeatureValue type.\n",
    "        \"\"\"\n",
    "        payloads = []\n",
    "        for row in new_records:\n",
    "            payload = aiplatform_v1beta1.WriteFeatureValuesPayload()\n",
    "            payload.entity_id = row.TERMINAL_ID\n",
    "            payload.feature_values = {\n",
    "                \"terminal_id_nb_tx_15min_window\": aiplatform_v1beta1.FeatureValue(int64_value=aggregated.TERMINAL_ID_NB_TX_15MIN_WINDOW),\n",
    "                \"terminal_id_nb_tx_30min_window\": aiplatform_v1beta1.FeatureValue(int64_value=aggregated.TERMINAL_ID_NB_TX_30MIN_WINDOW),\n",
    "                \"terminal_id_nb_tx_60min_window\": aiplatform_v1beta1.FeatureValue(int64_value=aggregated.TERMINAL_ID_NB_TX_60MIN_WINDOW),\n",
    "                \"terminal_id_avg_amount_15min_window\": aiplatform_v1beta1.FeatureValue(double_value=aggregated.TERMINAL_ID_SUM_AMOUNT_15MIN_WINDOW / aggregated.TERMINAL_ID_NB_TX_15MIN_WINDOW),\n",
    "                \"terminal_id_avg_amount_30min_window\": aiplatform_v1beta1.FeatureValue(double_value=aggregated.TERMINAL_ID_SUM_AMOUNT_30MIN_WINDOW / aggregated.TERMINAL_ID_NB_TX_30MIN_WINDOW),\n",
    "                \"terminal_id_avg_amount_60min_window\": aiplatform_v1beta1.FeatureValue(double_value=aggregated.TERMINAL_ID_AVG_AMOUNT_60MIN_WINDOW),\n",
    "            }\n",
    "            payloads.append(payload)\n",
    "        return payloads\n",
    "    \n",
    "    def send_request_to_feature_store(self, resource_name: str, payloads: List[Any]):\n",
    "        \"\"\"\n",
    "        Sends a write request to vertex ai feature store by preparing \n",
    "        a write feature value request using provided resource name and payloads, and \n",
    "        by makeing use of a feature store online serving service client\n",
    "        \"\"\"\n",
    "        # Prepare request\n",
    "        request = aiplatform_v1beta1.WriteFeatureValuesRequest(\n",
    "            entity_type=resource_name,\n",
    "            payloads=payloads,\n",
    "        )\n",
    "\n",
    "        # Create feature store online serving service client\n",
    "        client_options = {\n",
    "            \"api_endpoint\": \"us-central1-aiplatform.googleapis.com\"\n",
    "        }\n",
    "        v1beta1_client = aiplatform_v1beta1.FeaturestoreOnlineServingServiceClient(client_options=client_options)\n",
    "        \n",
    "        # Send the request\n",
    "        response = v1beta1_client.write_feature_values(request=request)\n",
    "        return response\n",
    "\n",
    "\n",
    "    def process(self, element: Tuple) -> Tuple:\n",
    "        \"\"\"\n",
    "        Select entity using resource_name variable and \n",
    "        write the respective features to Vertex AI feature store\n",
    "        \"\"\"\n",
    "        new_records = element[1]['new_records']\n",
    "        aggregated = element[1]['aggregated'][0]\n",
    "        \n",
    "        entity = self.resource_name.split(\"/\")[-1]\n",
    "        payloads = []\n",
    "        if entity == \"customer\":\n",
    "            payloads = self.populate_customer_payload(new_records, aggregated)\n",
    "        elif entity == \"terminal\":\n",
    "            payloads = self.populate_terminal_payload(new_records, aggregated)            \n",
    "\n",
    "        response = self.send_request_to_feature_store(self.resource_name, payloads)\n",
    "        yield (response,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the pipeline\n",
    "\n",
    "Now we are ready to build the pipeline using the defined classes and functions above. Once the pipeline is ready, we will wrap everything into a main function and submit it to the Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to beam_pipeline/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a beam_pipeline/main.py\n",
    "\n",
    "def main():\n",
    "    # Initialize google ai platform\n",
    "    aiplatform.init(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION\n",
    "    )\n",
    "\n",
    "    # Get entitye types for customer and terminal\n",
    "    fs = aiplatform.featurestore.Featurestore(\n",
    "        featurestore_name=FEATURESTORE_ID\n",
    "    )\n",
    "    customer_entity_type = fs.get_entity_type(\"customer\")\n",
    "    terminal_entity_type = fs.get_entity_type(\"terminal\")\n",
    "    \n",
    "    # Setup pipeline options for deploying to dataflow\n",
    "    pipeline_options = PipelineOptions(streaming=True, \n",
    "                                       save_main_session=True,\n",
    "                                       runner=\"DataflowRunner\",\n",
    "                                       project=PROJECT_ID,\n",
    "                                       region=REGION,\n",
    "                                       temp_location=f\"gs://{BUCKET_NAME}/dataflow/tmp\",\n",
    "                                       requirements_file=\"requirements.txt\",\n",
    "                                       max_num_workers=2)\n",
    "    \n",
    "    # Build pipeline and transformation steps\n",
    "    pipeline = beam.Pipeline(options=pipeline_options)\n",
    "    \n",
    "    source = (\n",
    "        pipeline\n",
    "        | 'Read from Pub/Sub' >> beam.io.ReadFromPubSub(subscription=SUBSCRIPTION_PATH)\n",
    "        | 'Decode byte array to json dict' >> beam.Map(lambda row: json.loads(row.decode('utf-8')))\n",
    "    )\n",
    "\n",
    "    enriched_source = (\n",
    "        source\n",
    "        | 'Attach timestamps' >> beam.Map(lambda row: beam.window.TimestampedValue(row, to_unix_time(row['TX_TS'])))\n",
    "        | 'Create sliding window' >> beam.WindowInto(beam.window.SlidingWindows(WINDOW_SIZE, WINDOW_PERIOD, offset=WINDOW_SIZE))\n",
    "        | 'Add window info' >> beam.ParDo(AddAddtionalInfo())\n",
    "        | 'Convert to namedtuple' >> beam.Map(lambda row: beam.Row(**row))\n",
    "    )\n",
    "\n",
    "    new_records = (\n",
    "        enriched_source\n",
    "        | 'Filter only new rows' >> beam.Filter(lambda row: row.TS_DIFF <= WINDOW_PERIOD)\n",
    "    )\n",
    "\n",
    "    # Engineer customer features\n",
    "    new_records_customer_id = (\n",
    "        new_records\n",
    "        | 'Assign CUSTOMER_ID_COMPOSITE_KEY as key' >> beam.WithKeys(lambda row: row.CUSTOMER_ID_COMPOSITE_KEY)\n",
    "    )\n",
    "\n",
    "    aggregated_customer_id = (\n",
    "        enriched_source\n",
    "        | 'Group by customer id composite key column' >> beam.GroupBy(CUSTOMER_ID_COMPOSITE_KEY='CUSTOMER_ID_COMPOSITE_KEY')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= SECONDS_WINDOWS[0] else 0, sum, 'CUSTOMER_ID_NB_TX_15MIN_WINDOW')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= SECONDS_WINDOWS[1] else 0, sum, 'CUSTOMER_ID_NB_TX_30MIN_WINDOW')\n",
    "            .aggregate_field('TX_ID', CountCombineFn(), 'CUSTOMER_ID_NB_TX_60MIN_WINDOW')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= SECONDS_WINDOWS[0] else 0, sum,'CUSTOMER_ID_SUM_AMOUNT_15MIN_WINDOW')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= SECONDS_WINDOWS[1] else 0, sum, 'CUSTOMER_ID_SUM_AMOUNT_30MIN_WINDOW')\n",
    "            .aggregate_field('TX_AMOUNT', MeanCombineFn(), 'CUSTOMER_ID_AVG_AMOUNT_60MIN_WINDOW')\n",
    "        | 'Assign key for aggregated results (customer id)' >> beam.WithKeys(lambda row: row.CUSTOMER_ID_COMPOSITE_KEY)\n",
    "    )\n",
    "\n",
    "    merged_customer_id = (\n",
    "        ({\n",
    "            'new_records': new_records_customer_id, \n",
    "            'aggregated': aggregated_customer_id\n",
    "        })\n",
    "        | 'Merge pcollections (customer id)' >> beam.CoGroupByKey()\n",
    "        | 'Filter empty rows (customer id)' >> beam.Filter(lambda row: len(row[1]['new_records']) > 0)\n",
    "        | 'Write to feature store (customer id)' >> beam.ParDo(WriteFeatures(customer_entity_type.resource_name))\n",
    "    )\n",
    "\n",
    "    # Engineer terminal features\n",
    "    new_records_terminal_id = (\n",
    "        new_records\n",
    "        | 'Assign TERMINAL_ID_COMPOSITE_KEY as key' >> beam.WithKeys(lambda row: row.TERMINAL_ID_COMPOSITE_KEY)\n",
    "    )\n",
    "\n",
    "    aggregated_terminal_id = (\n",
    "        enriched_source\n",
    "        | 'Group by terminal id composite key column' >> beam.GroupBy(TERMINAL_ID_COMPOSITE_KEY='TERMINAL_ID_COMPOSITE_KEY')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= SECONDS_WINDOWS[0] else 0, sum, 'TERMINAL_ID_NB_TX_15MIN_WINDOW')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= SECONDS_WINDOWS[1] else 0, sum, 'TERMINAL_ID_NB_TX_30MIN_WINDOW')\n",
    "            .aggregate_field('TX_ID', CountCombineFn(), 'TERMINAL_ID_NB_TX_60MIN_WINDOW')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= SECONDS_WINDOWS[0] else 0, sum, 'TERMINAL_ID_SUM_AMOUNT_15MIN_WINDOW')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= SECONDS_WINDOWS[1] else 0, sum, 'TERMINAL_ID_SUM_AMOUNT_30MIN_WINDOW')\n",
    "            .aggregate_field('TX_AMOUNT', MeanCombineFn(), 'TERMINAL_ID_AVG_AMOUNT_60MIN_WINDOW')\n",
    "        | 'Assign key for aggregated results (terminal id)' >> beam.WithKeys(lambda row: row.TERMINAL_ID_COMPOSITE_KEY)\n",
    "    )\n",
    "\n",
    "    merged_terminal_id = (\n",
    "        ({\n",
    "            'new_records': new_records_terminal_id, \n",
    "            'aggregated': aggregated_terminal_id\n",
    "        })\n",
    "        | 'Merge pcollections (terminal id)' >> beam.CoGroupByKey()\n",
    "        | 'Filter empty rows (terminal id)' >> beam.Filter(lambda row: len(row[1]['new_records']) > 0)\n",
    "        | 'Write to feature store (terminal id)' >> beam.ParDo(WriteFeatures(terminal_entity_type.resource_name))\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline (async)\n",
    "    pipeline.run()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `requirement.txt` for Dataflow Workers\n",
    "\n",
    "As we are using `google-cloud-aiplatform` package, we need to pass the `requirement.txt` to the Dataflow Workers so that the workers will install the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting beam_pipeline/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile beam_pipeline/requirements.txt\n",
    "\n",
    "google-cloud-aiplatform\n",
    "google-apitools==0.5.32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the pipeline\n",
    "\n",
    "Now we are ready to deploy this pipeline to Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 beam_pipeline/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1_data_engineering_sdk.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m96"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
